\documentclass[a4paper,14pt]{extarticle}
\usepackage[left = 20mm, right = 20mm, top = 25mm, bottom = 25mm]{geometry}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{euscript}
\usepackage{graphics}
\usepackage[T1, T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage{pgf,tikz}
\usepackage{multicol}
\usepackage{fancyvrb}
\usepackage{fvextra}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancyhdr} 
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[LE,RO]{Университет ИТМО}
\fancyhead[RE,LO]{Высшая школа цифровой культуры}
\fancyfoot[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{2pt}
 
% Добавляем свои блоки
 
\makeatletter
\algblock[ALGORITHMBLOCK]{BeginAlgorithm}{EndAlgorithm}
\algblock[BLOCK]{BeginBlock}{EndBlock}

\makeatother
 
% Нумерация блоков
 
\usepackage{caption}% http://ctan.org/pkg/caption
\captionsetup[ruled]{labelsep=period}
\renewcommand{\thealgorithm}{\arabic{algorithm}}

\sloppy

\usepackage{xcolor} % Required for specifying custom colours
\definecolor{grey}{rgb}{0.9,0.9,0.9} % Colour of the box surrounding the title
%\usepackage[sfdefault]{ClearSans} % Use the Clear Sans font (sans serif)
%\usepackage{XCharter} % Use the XCharter font (serif)
\definecolor{foo}{HTML}{EFF5F9}

\newcommand{\eqdef}{\overset{\mathrm{def}}{=\joinrel=}}
\def\re{{\rm Re}}
\def\im{{\rm Im}}
\def\dim{\rm dim}
\def\Ext{\rm Ext}
\def\wt#1{{{\widetilde #1} }}
\def\wh#1{{{\,\widehat #1\,} }}
\def\graph{{\rm gr\,}}
\def\ran{{\rm ran\,}}
\def\dom{{\rm dom\,}}
\def\ker{{\rm ker\,}}
\def\supp{{\rm supp\,}}
\def\diag{{\rm diag\,}}

\newcommand\dN{{\mathbb{N}}}
\newcommand\dR{{\mathbb{R}}}
\newcommand\dC{{\mathbb{C}}}
\newcommand{\bO}{{\mathbb{O}}}
\newcommand{\bU}{{\mathbb{U}}}
\newcommand\dZ{{\mathbb{Z}}}

\newcommand\gotB{{\mathfrak{B}}}
\newcommand\gotD{{\mathfrak{D}}}
\newcommand\gotH{{\mathfrak{H}}}
\newcommand\gotK{{\mathfrak{K}}}
\newcommand\gotL{{\mathfrak{L}}}
\newcommand\gotM{{\mathfrak{M}}}
\newcommand\gotN{{\mathfrak{N}}}
\newcommand\gotR{{\mathfrak{R}}}
\newcommand\gotS{{\mathfrak{S}}}
\newcommand\gotT{{\mathfrak{T}}}
\newcommand\gott{{\mathfrak{t}}}
\newcommand\gotC{{\mathfrak{C}}}
\newcommand\gotZ{{\mathfrak{Z}}}
\newcommand{\RNumb}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\newcommand{\ga}{{\alpha}}
\newcommand{\gd}{{\delta}}
\newcommand{\gD}{{\Delta}}
\newcommand{\gga}{{\gamma}}
\newcommand{\gG}{{\Gamma}}
\newcommand{\gF}{{\Phi}}
\newcommand{\gf}{{\phi}}
\newcommand{\gk}{{\kappa}}
\newcommand{\gK}{{\Kappa}}
\newcommand{\gl}{{\lambda}}
\newcommand{\gL}{{\Lambda}}
\newcommand{\gO}{{\Omega}}
\newcommand{\go}{{\omega}}
\newcommand{\gs}{{\sigma}}
\newcommand\gS{{\Sigma}}
\newcommand{\gT}{{\Theta}}
\newcommand{\gY}{{\Upsilon}}

\newcommand\cA{{\mathcal{A}}}
\newcommand\cB{{\mathcal{B}}}
\newcommand\cC{{\mathcal{C}}}
\newcommand\cD{{\mathcal{D}}}
\newcommand\cH{{\mathcal{H}}}
\newcommand\cK{{\mathcal{K}}}
\newcommand\cM{{\mathcal{M}}}
\newcommand\cN{{\mathcal{N}}}
\newcommand\cO{{\mathcal{O}}}
\newcommand\cP{{\mathcal{P}}}
\newcommand\cT{{\mathcal{T}}}
\newcommand\cU{{\mathcal{U}}}
\newcommand\cZ{{\mathcal{Z}}}

\newcommand\Rg{{\rm Rg}}
\newcommand\res{{\rm res}}
\newcommand\pr{{\rm Pr}}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Argmax}{Arg\,max}

\newtheorem{theorem}{Теорема}[subsection]
\newtheorem{lemma}{Лемма}[subsection]
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{definition}{Определение}[subsection]
\newtheorem{example}{Пример}[subsection]
\newtheorem{remark}{Замечание}[subsection]

\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}

\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\bead}{\begin{eqnarray*}}
\newcommand{\eead}{\end{eqnarray*}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\bed}{\begin{displaymath}}
\newcommand{\eed}{\end{displaymath}}

\newcommand{\bl}{\begin{lemma}}
\newcommand{\el}{\end{lemma}}

\newcommand{\bt}{\begin{theorem}}
\newcommand{\et}{\end{theorem}}

\newcommand{\bc}{\begin{corollary}}
\newcommand{\ec}{\end{corollary}}

\newcommand{\br}{\begin{remark}}
\newcommand{\er}{\end{remark}}

\newcommand{\bd}{\begin{definition}}
\newcommand{\ed}{\end{definition}}

\newcommand{\bspi}{\begin{split}}
\newcommand{\espi}{\end{split}}

\newcommand{\la}{\label}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}


\newenvironment{proof}%
{\begin{sloppypar}\noindent{\bf Доказательство.}}%
{\hspace*{\fill}$\square$\end{sloppypar}}
\renewcommand{\Large}{\fontsize{16}{25pt}\selectfont}

\newcommand{\slim}{\,\mbox{\rm s-}\hspace{-2pt} \lim}
\newcommand{\wlim}{\,\mbox{\rm w-}\hspace{-2pt} \lim}
\newcommand{\olim}{\,\mbox{\rm o-}\hspace{-2pt} \lim}
\newcommand{\transpose}[1]{\ensuremath{#1^{\scriptscriptstyle t}}}

\graphicspath{ {img/} }

\usepackage{indentfirst} 
\setlength\parindent{1cm}
% НАСТРОЙКИ ЗАГОЛОВКОВ
\usepackage{textcase} 
\usepackage{titlesec}
\titleformat{\section}[block]{\sffamily\Large\bfseries\filcenter}{\thesection}{0.5em}{}
\titleformat{\subsection}[block]{\large\sffamily\bfseries}{\thesubsection}{0.5em}{}
\titlespacing{\subsection}{2cm}{1mm}{3mm}

\begin{document}
%ДЛЯ АЛГОРИТМОВ
	% Перевод данных об алгоритмах
	\renewcommand{\listalgorithmname}{Список алгоритмов}
	\floatname{algorithm}{Алгоритм}
	
\algnewcommand{\Init}[1]{%
  \State \textbf{Инициализировать:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

	
	% Перевод команд псевдокода
	\algrenewcommand\algorithmicwhile{\textbf{Повторять пока }}
	\algrenewcommand\algorithmicdo{\textbf{Выполнять}}
	\algrenewcommand\algorithmicrepeat{\textbf{Повторять}}
	\algrenewcommand\algorithmicuntil{\textbf{Пока выполняется}}
	\algrenewcommand\algorithmicend{\textbf{Конец}}
	\algrenewcommand\algorithmicif{\textbf{Если}}
	\algrenewcommand\algorithmicelse{\textbf{иначе}}
	\algrenewcommand\algorithmicthen{\textbf{тогда}}
	\algrenewcommand\algorithmicfor{\textbf{Для}}
	\algrenewcommand\algorithmicforall{\textbf{Выполнить для всех}}
	\algrenewcommand\algorithmicfunction{\textbf{Функция}}
	\algrenewcommand\algorithmicprocedure{\textbf{Процедура}}
	\algrenewcommand\algorithmicloop{\textbf{повторять}}
	\algrenewcommand\algorithmicrequire{\textbf{Вход:}}
	\algrenewcommand\algorithmicensure{\textbf{Обеспечивающие условия:}}
	\algrenewcommand\algorithmicreturn{\textbf{Возвратить}}
	\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
	\algrenewtext{EndWhile}{\textbf{Конец цикла}}
	\algrenewtext{EndLoop}{\textbf{Конец}}
	\algrenewtext{EndFor}{\textbf{конец цикла}}
	\algrenewtext{EndFunction}{\textbf{Конец функции}}
	\algrenewtext{EndProcedure}{\textbf{Конец процедуры}}
	\algrenewtext{EndIf}{\textbf{Конец условия}}
	\algrenewtext{EndFor}{\textbf{Конец цикла}}
	\algrenewtext{BeginAlgorithm}{\textbf{Начало алгоритма}}
	\algrenewtext{EndAlgorithm}{\textbf{Конец алгоритма}}
	\algrenewtext{BeginBlock}{\textbf{Начало блока. }}
	\algrenewtext{EndBlock}{\textbf{Конец блока}}
	\algrenewtext{ElsIf}{\textbf{иначе если }}
	
	\newcommand{\algorithmicbreak}{\textbf{Конец цикла}}
    \newcommand{\BREAK}{\STATE \algorithmicbreak}



%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	
	%------------------------------------------------
	%	Grey title box
	%------------------------------------------------
	\begin{center}
	\includegraphics[width=0.4\textwidth]{dc.png}
    \end{center}
    
    	\vfill
    	
	\colorbox{foo}{
		\parbox[t]{0.93\textwidth}{ % Outer full width box
			\parbox[t]{0.91\textwidth}{ % Inner box for inner right text margin
				\raggedleft % Right align the text
				%\fontsize{50pt}{80pt}\selectfont % Title font size, the first argument is the font size and the second is the line spacing, adjust depending on title length
				\vspace{0.7cm} % Space between the start of the title and the top of the grey box
				
				\huge Обучение с подкреплением\\
				
				\vspace{0.7cm} % Space between the end of the title and the bottom of the grey box
			}
		}
	}
	
	\vfill % Space between the title box and author information
	
	%------------------------------------------------
	%	Author name and information
	%------------------------------------------------

    

	
	\parbox[t]{0.93\textwidth}{ % Box to inset this section slightly
		\raggedleft % Right align the text
		%\large % Increase the font size
		{Высшая Школа Цифровой Культуры}\\[4pt] % Extra space after name
		Университет ИТМО\\[4pt] % Extra space before URL
		dc@itmo.ru\\[4pt]
		
		\hfill\rule{0.2\linewidth}{1pt}% Horizontal line, first argument width, second thickness
	}
	
\end{titlepage}

%----------------------------------------------------------------------------------------

\newpage
%
\pagestyle{empty}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\tableofcontents
%
\clearpage
\pagestyle{fancy}


\section{Понятие обучения с подкреплением}
\subsection{Базовые определения и постановка задачи}
Здравствуйте, уважаемые слушатели! В этой лекции мы обсудим еще одну ветку, или еще один тип машинного обучения -- обучение с подкреплением. Но начнем, конечно же, с мотивировки и отличий от тех типов, что уже были изучены ранее.

Итак, давайте вспомним с чем мы имеем дело, решая, скажем, задачу обучения с учителем? У нас есть набор предикторов (или входных переменных) $X_1, X_2, ..., X_p$ и набор откликов $Y$ (или набор правильных ответов). Нашей задачей является обобщение <<имеющегося опыта>> (опыта, основанного на тренировочных данных) на <<все пространство>> (на произвольные тестовые данные). Например, решая задачу регрессии при $p = 1$, мы имеем в руках лишь (грубо говоря) набор из $n$ пар данных $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$. Наша же цель -- построить зависимость вида $Y = f(X)$, которая способна предсказать <<правильный ответ>> уже для любого $X$ из возможного множества значений предиктора $X_1$. 

Аналогично и с задачей классификации: просто вспомните, как, зная классы лишь конечного числа тренировочных объектов, в результате классификации все пространство признаков (с бесконечным числом объектов) <<раскрашивается>> в разные цвета, отвечающие тому или иному классу. Вот оно -- обобщение на все пространство. Понятно, что аналогичные рассуждения справедливы и при рассмотрении обучения без учителя. 

Однако в реальной жизни обучение происходит далеко не всегда по схеме, в которой известны <<верные ответы>>. Скажем, разработка учеными нового лекарственного препарата, или вывод математиками очередной хитрой формулы, -- это процессы, не имеющие откликов, кроме финального: работает препарат или нет, получилась формула или нет. Сам же процесс создания (будь то формулы или препарата), скорее всего, описывается примерно так: мы что-то делаем, в результате наших действий что-то происходит, этот результат мы как-то оцениваем, обдумываем, а затем продолжаем что-то делать, стараясь приблизиться к желаемому результату. Согласитесь, под такую вольно описанную логику подходит куча различных процессов, встречающихся в повседневной жизни на каждом шагу:
\begin{enumerate}
\item Например, а как ребенок учится вставать, научившись ползать (описание процесса может быть не очень точным)? Сначала он предпринимает множество попыток встать с колен, но, по понятным причинам, постоянно падает. Однако через какое-то время он понимает (конечно, из опыта), что когда его поддерживает рука мамы, то встать гораздо проще. Значит, нужно за что-то держаться, вставая. Стол, стул, да хотя бы стена -- все подойдет. Остается только немного покарабкаться, и вуаля -- человек научился вставать.
\item А как проходит футбольный матч? Конечно, у каждой команды есть заранее продуманная тактика, учитывающая как сильные, так и слабые стороны как соперников, так и себя (кстати, наработанная опытом). Но ход игры слишком непредсказуем: почему-то именно сегодня нападающий достаточно легко может попасть к воротам соперника именно со стороны правого фланга. Видимо, защитники думают о чем-то другом. Что же, это <<на руку>>, значит по этому флангу и имеет смысл в основном атаковать.
\end{enumerate}
Конечно, можно привести еще множество различных примеров, но, скорее всего, идея ясна. Давайте теперь подойдем к некоторой формальной постановке задачи и введем определения, которые будем повсеместно использовать в дальнейшем.

Итак, говоря несколько грубо, обучение с подкреплением -- это раздел машинного обучения, основывающийся на следующей парадигме: обучаемая модель не имеет детальных сведений об окружающей ее системе, однако может производить в ней (в этой системе) какие-то действия, анализируя реакцию системы на эти действия. Иными словами, анализируя подкрепления. Ясно, что как модель воздействует на систему, так и система воздействует на модель, поэтому пару система-модель разлучать нельзя. Так что следующее определение носит весьма условный характер.

\begin{definition}
Модель, обучающуюся в рамках задачи обучения с подкреплением, часто называют агентом. Окружающую агента систему часто называют средой. 
\end{definition} 
Перейдем к формальному описанию процесса взаимодействия агента и среды. 
\begin{enumerate}
	\item Пусть агент и среда взаимодействуют в дискретные моменты времени $t \in \{0, 1, 2, ..., n\}$,  $n \in \mathbb{N}\cup \{0, \infty\}$: количество взаимодействий может быть как конечным (если $n$ конечно), так и бесконечным.  
	\item На каждом шаге агент находится в каком-то состоянии $s_t \in S$, где $S$ -- множество всех возможных состояний. 
	\item На основании состояния $s_t$, агент выбирает некоторое действие $a_t \in A(s_t) $, где $A(s_t)$ -- множество действий, доступных агенту в состоянии $s_t$. 
	\item Совершив действие $a_t$, среда генерирует награду $r_{t + 1}$ и переводит агента в состояние $s_{t+1}$. 
	\item Шаги 3-4 повторяются, если не достигнут какой-то критерий остановки.
\end{enumerate}
В дальнейшем мы подробно поясним, что означают слова <<генерирует>>, <<переводит>>, и   опишем всю схему математически корректно. Сейчас же, чтобы закрепить общее понимание  происходящего, приведем весьма условный пример диалога между средой и агентом:
\begin{enumerate}
\item \textbf{Среда:} Агент, ты в состоянии $s_0$. Тебе доступны действия с номерами $3, 4, 5$ ($A(s_0) = \{3, 4, 5\}$).\\
\textbf{Агент:} Совершаю действие с номером $4$ ($a_0 = 4$).
\item \textbf{Среда:} Ты получаешь награду в $3$ единицы ($r_1 = 3$) и переходишь в состояние $s_1$. Теперь тебе доступны действия с номерами $1, 4, 5$ ($A(s_1) = \{1, 4, 5\}$). \\
\textbf{Агент:} Совершаю действие с номером $5$ ($a_1 = 5$). 
\item \textbf{Среда:} Ты получаешь награду в $-1$ единицу ($r_2 = -1$) и переходишь в состояние $s_2$. Тебе доступны ... Ну и так далее. 
\end{enumerate}
Как мы видим, награды (или подкрепления) могут быть как положительными, так и отрицательными. Задача агента -- максимизировать накопительную награду, опирается на следующую гипотезу (так называемую гипотезу о вознаграждении -- Reward Hypothesis): <<Все цели могут быть описаны путем максимизации ожидаемого совокупного вознаграждения>> (<<All goals can be described by the maximisation of expected cumulative reward>>). О том, что понимается под этими словами, мы поговорим чуть позже.

Итак, надеемся, что схема взаимодействия прозрачна и ясна. Однако остается еще одна проблема, решению которой, по большому счету, и будет посвящено все дальнейшее изложение: как выбирать действие $a_t$ на шаге $t$ из набора возможных действий $A(s_t)$? Так мы приходим к понятию стратегии.
\begin{definition}
	Набор вероятностей $\pi_t(a|s_t)$ выбора доступного действия $a$ в момент времени $t$ (в случае, когда агент находится в состоянии $s_t \in S$), называется стратегией агента.
\end{definition}
Иными словами, в случае, когда в состоянии $s_t$ в момент времени $t$ доступно более одного действия в наборе $A(s_t)$, выбор совершаемого действия производится вероятностным образом: согласно стратегии $\pi_t(a|s_t)$. Как же задать такую стратегию, которая максимизирует ожидаемое совокупное вознаграждение? Это и есть тот самый <<вопрос на миллион>>, который мы и будем обсуждать в дальнейшем.


\subsection{Компромисс изучение-применение}
Перед тем как переходить к рассмотрению конкретных примеров и задач, обратим наше внимание на так называемую проблему (или конфликт, компромисс) между изучением и применением (<<exploration-exploitation>>). Принцип применения основывается на следующем предложении: на каждом шаге предлагается принять наилучшее (или наиболее выгодное) решение на основе имеющейся информации. Принцип изучения меняет концепцию: собрать как можно больше информации для (возможно) более успешного применения в дальнейшем. Идею упомянутого конфликта достаточно просто понять из рисунка \ref{mouse}.
\begin{figure}[h!]
	\center{\includegraphics[width=.7\linewidth]{img/mouse.png}}
	\caption{Компромисс <<изучение-применение>>}
	\label{mouse}
\end{figure}

Предположим, что за один ход мышонок (наш агент) совершает одно перемещение (действие). Игра останавливается либо когда мышонок съедает сыр, либо когда попадает в ловушку, либо когда у него заканчивается   запас ходов (пусть всего их, скажем, $10$ ). Производится некоторое конечное количество экспериментов (игр), в результате чего подсчитывается общее количество добытого сыра. Ясно, что чем больше в результате всех игр добыто сыра -- тем лучше. Кроме того, не добраться до сыра (истратить все ходы) и не попасть в ловушку (не убить агента) -- результат, который намного лучше, чем попасть в ловушку (то есть убить агента). 

Из рисунка очевидно, что мышонку проще (и, вероятно, быстрее, а также безопаснее) всего каждый раз добираться до маленького кусочка сыра, расположенного поблизости, -- так он гарантированно будет получать хоть и маленькую, но награду в результате каждой игры. При таком подходе (который часто нарекается подходом <<лучше синица в руках>>) внушительные запасы сыра, лежащие рядом с ловушками, обесцениваются вовсе. Описанное является ничем иным, как принципом <<применения>>.

С другой стороны, мышонок может пожертвовать сиюминутным результатом (и, быть может, даже жизнью), чтобы исследовать окружающую среду и попытаться найти пусть и более сложную и опасную дорогу, но к более значимой награде, а значит в итоге увеличить добычу сыра (или награды) <<в среднем>>.

Итак, тезисно компромисс между изучением и применением может быть охарактеризован следующими пунктами:
\begin{enumerate}
\item Необходимо получить достаточно информации об окружающей среде, чтобы решения (а значит и результаты) были наилучшими <<в среднем>>. 
\item Лучшая долгосрочная стратегия допускает (и даже обязательно включает в себя) краткосрочные потери.
\end{enumerate} 

Описанная проблема между изучением и применением -- не что-то, созданное на пустом месте. Эта проблема регулярно возникает и в реальной жизни. Например, пусть вы решили ужинать в ресторане. Применение -- это выбор проверенного ресторана, который стабильно не подводит ни качеством блюд, ни их ценой. Изучение -- это выбор и опробование какого-то нового заведения: а вдруг там окажется лучше? 

Ну что, надеемся, что как проблема, так и постановка задачи обучения с подкреплением ясна и прозрачна. Перейдем теперь к разбору одной из самых понятных и популярных задач -- к задаче о $k$-руком бандите.
 

\subsection{Постановка задачи о $k$-руком бандите}
Как мы уже сказали, одним из самых простых примеров, иллюстрирующих идею обучения с подкреплением, является так называемая задача о $k$-руком бандите. Название задачи происходит из аналогии с игральным автоматом (хорошо известным, как <<однорукий бандит>>). В нашем же случае будем считать, что мы имеем дело с автоматом, имеющим $k$ ручек. 

Давайте для начала определимся: что в этой задаче является агентом, что средой, что стратегией и так далее. Агент -- это человек, которому в каждый момент времени $t$ в любом состоянии $s \in S$ доступно любое из $k$ одних и тех же действий -- выбрать одну из $k$ ручек. Конечно, каждый раз этот выбор осуществляется в соответствии с какой-то своей стратегией $\pi_t$. Выбрав действие $a_t$ в момент времени $t$, агент получает заранее неизвестное вознаграждение $r_{t+1}$, диктуемое средой -- $k$-руким автоматом, и переходит в следующее состояние.
\begin{remark}
Отметим отдельно, что так как возможности (или набор возможных действий $A(s)$) агента в каждом состоянии $s \in S$ одинаковы -- выбор одной из $k$ ручек, то хочется сказать, что и состояния $s$ агента на каждом шаге одинаковы (или вообще, что состояние всего одно). Такое предположение, вообще говоря, неверно, так как в каждом состоянии $s_t \in S$ в момент времени $t$, агент действует согласно некоторой, вообще говоря, различной стратегии $\pi_t$.
\end{remark}
Теперь давайте обратимся к естественному вопросу: из каких соображений дергать за ручки? Если бы мы заранее знали ценность каждого действия, то логичной жадной (недальновидной) стратегией была бы такая: в момент времени $t$ выбрать то действие (ту ручку), вознаграждение $r_{t+1}$ за выбор которой будет максимальным. Мы же будущего не знаем, поэтому способны оценивать только результат наших прошлых действий: <<дернул>> за эту ручку -- получил столько-то, за другую -- столько-то, и так далее. В итоге оказывается разумным ввести следующее определение. 
\begin{definition}
Пусть $q_0(a) \in \mathbb R$ -- начальная оценка ценности действия $a$ (устанавливается исследователем).Оценкой ценности действия $a$ в момент времени $t \geq 1$ называется величина
$$
q_t(a) = 
\begin{cases}
q_0(a), & \text{\emph{действие $a$ не выбрано ни разу}} \\
\\
\displaystyle{\frac{\sum\limits_{i=0}^{t-1}r_{i+1}\mathsf{I}(a_i = a)}{ \sum\limits_{i=0}^{t-1} \mathsf{I}(a_i = a)}}, & \text{\emph{иначе}}
\end{cases},
$$
где $\mathsf{I}(a_i = a)$ -- индикатор события $a_i = a$: он равен единице, если на $i$-ом шаге выбрано действие $a$, и нулю иначе. 

%Начальные ценности $q_0(a) \in \mathbb R$ могут быть установлены произвольно (например, равняться нулю). %В случае, если до шага $t$ событие $a$ не было выбрано ни разу, в качестве значения $q_t(a)$ может быть выбрано любое число, например $0$. 
%В случае, если до шага $t$ действие $a$ не было выбрано ни разу, в качестве значения $q_t(a)$ используется $q_0(a)$.
\end{definition}
\begin{remark}
Написанное выражение (в случае, если действие $a$ выбрано хотя бы один раз) --среднее арифметическое вознаграждений, полученных при выборе действия $a$ до момента времени $t$. 
\end{remark}
Рассмотрим пример.
\begin{example}
Пусть в процессе некоторой игры агент на каждом шаге совершает одно из двух возможных действий: $a$ или $b$, в соответствии с некоторой стратегией, и на каждом шаге $t$ получает некоторую награду $r_{t+1}$. Данные (с использованием введенных выше обозначений) представлены в таблице.
\begin{center}
\begin{tabular}{ || c | c | c || }
\hline
\textbf{Шаг} & \textbf{Действие} & \textbf{Награда}  \\ \hline
$t = 0$ & $a_0 = b$ & $r_1 = 3$ \\ \hline
$t = 1$ & $a_1 = a$ & $r_2 = 0$  \\ \hline
$t = 2$ & $a_2 = a$ & $r_3 = 8$ \\ \hline
$t = 3$ & $a_3 = b$ & $r_4 = 1$  \\ \hline
$t = 4$ & $a_4 = b$ & $r_5 = 5$ \\
\hline
\end{tabular}
\end{center}
Найдем оценки ценностей действий $a$ и $b$ на пятом шаге.
Действие $a$ было выбрано $2$ раза, при этом были получены награды $0$ и $8$. Тогда оценка ценности действия $a$ на пятом шаге может быть найдена следующим образом (как мы и говорили -- как среднее арифметическое ранее полученных вознаграждений):
$$
q_5(a) = \frac{0+8}{2} = 4.
$$
Конечно, то же самое получится и при использовании <<оригинального>> соотношения из определения:
$$
q_5(a) = \frac{\sum\limits_{i=0}^{5-1}r_{i+1}\mathsf{I}(a_i = a)}{ \sum\limits_{i=0}^{5-1} \mathsf{I}(a_i = a)} = \frac{3 \cdot 0 + 0 \cdot 1 + 8 \cdot 1 + 1 \cdot 0 + 5 \cdot 0}{0 + 1 + 1 + 0 + 0} = 4.
$$
Аналогичным образом найдем оценку ценности действия $b$ на пятом шаге: 
$$
q_5(b) = \frac{3+1+5}{3} = 3.
$$
Можно заметить, что оценка ценности действия $a$, посчитанная на пятом шаге, выше, чем оценка ценности действия $b$, однако все может измениться, если накопить большее количество информации о получаемых наградах. 
\end{example}
 
Использование описанного подхода при оценке ценности влечет некоторые издержки с точки зрения вычислений и хранения информации. Нетрудно заметить, что для оценки ценности приходится хранить информацию обо всех полученных ранее вознаграждениях. Резонно задуматься, а существует ли менее затратный способ? Оказывается, существует. Он дается следующей леммой. 
\begin{lemma}[Рекуррентная формула оценки ценности]
	Пусть действие $a$ в течение $(t - 1)$ шагов выбрано ровно $n \leq (t-1)$ раз, $q_t(a)$ -- оценка его ценности на шаге $t$. Если оно выбрано на шаге $t$, и за него получено вознаграждение $r_{t + 1}$, то	
$$
q_{t+1}(a) = q_t(a) + \frac{1}{n+1}\left(r_{t+1} - q_t(a)\right),
$$ 
иначе $q_{t + 1}(a) = q_t(a)$, $q_0(a) \in \mathbb R$ -- начальная оценка ценности.
\end{lemma}
\begin{remark}	
Использование введенной выше рекуррентной формулы позволяет хранить только два параметра: текущее значение ценности действия и число активаций этого действия.
\end{remark}
 
\begin{proof}
Случай, когда на шаге $t$ действие $a$ не выбрано, очевиден -- оценка ценности действия не меняется. Содержательным для рассмотрения является случай, когда на шаге $t$ действие $a$ все-таки выбрано.
По определению,
$$
q_t(a) = \frac{\sum\limits_{i=0}^{t-1}r_{i+1}\mathsf{I}(a_i = a)}{ \sum\limits_{i=0}^{t-1} \mathsf{I}(a_i = a)} = \frac{\sum\limits_{i=0}^{t-1}r_{i+1}\mathsf{I}(a_i = a)}{n},
$$
где последнее равенство справедливо в силу условия: действие $a$ до шага $t$ было выбрано $n$ раз. Но тогда $n q_t(a)$ -- это сумма вознаграждений, полученных за выбор действия $a$ до шага $(t - 1)$ включительно. Раз действие выбрано и на шаге $t$, то, так как за него получено вознаграждение $r_{t+1}$ (и теперь оно выбрано $(n + 1)$ раз), для его оценки ценности имеем
$$
q_{t+1}(a) = \frac{1}{n + 1}\left(r_{t+1} + nq_t(a) \right) = 
\frac{1}{n+1}\left(r_{t+1}+(n+1)q_{t}(a) - q_{t}(a)\right)=
 $$
 $$
  =q_{t}(a)+\frac{1}{n+1}\left(r_{t+1}-q_{t}(a)\right).
$$
\end{proof}
 
\begin{remark}
%Если в качестве оценки действия используют не среднее арифметическое полученных вознаграждений (в частности, чтобы динамически менять стратегию), часто используют и следующее правило получения $q_{t + 1}(a)$:
Помимо среднего арифметического полученных вознаграждений (в частности, чтобы динамически менять стратегию), часто используют и следующее правило получения $q_{t + 1}(a)$:
$$
q_{t + 1}(a) = q_{t}(a)  + \alpha_{t} \left(r_{t+1} - q_{t}(a)\right).
$$
В частности, если $\alpha_t = \alpha$ -- некоторая константа, то мы получаем экспоненциальное скользящее среднее:
$$
q_{t+1}(a) = \alpha r_{t + 1} + (1 - \alpha)q_t(a).
$$
Это соотношение показывает, что если $\alpha \in(0, 1)$, то наиболее ценными оказываются сиюминутные награды (принцип <<лучше синица в руках>>), а если $\alpha > 1$ -- наоборот. Сейчас мы на этом подробно останавливаться не будем.
\end{remark}

\subsection{Жадная стратегия}
Итак, мы переходим к рассмотрению различных стратегий выбора действий на шаге $t$. Наверное, первый и самый наивный способ максимизации награды -- это выбор на шаге $t$ того (или тех действий), оценка ценностей которого (или которых) максимальна. 
Пусть $A_t$ -- множество действий в состоянии $s_t$, обладающих максимальной (и, как следствие, одинаковой) оценкой ценности. Говоря математическим языком, множество $A_t$ -- это множество
$$
A_{t} = \Argmax\limits_{a \in A(s_t)}  q_{t}(a).
$$
Так как дальнейший выбор мы осуществляем только исходя из величины оценки ценности действия, а все действия из множества $A_t$ на шаге $t$ одинаково ценны (другие же не рассматриваются вовсе), то логично, что любое $a \in A_t$ имеет смысл выбирать равновероятно, а значит
$$
\pi_t(a|s_t) = \begin{cases}
	\frac{1}{|A_t|}, & a\in A_t\\
	0, & a \notin A_t
\end{cases},
$$
где $|A_t|$ -- количество элементов в множестве $A_t$. Таким образом, действия, обладающие максимальной оценкой ценности, выбираются с вероятностью $\frac{1}{|A_t|}$, остальные же действия игнорируются. 
\begin{definition}
	Стратегия, введенная выше, называется жадной стратегией.
\end{definition}

Можно рассмотреть и полную противоположность жадной стратегии -- так называемую абсолютно исследовательскую стратегию.
\begin{definition}
	Если в результате обучения на каждом шаге $t$ любое действие из множества $A(s_t)$ всех доступных в состоянии $s_t$ действий выбирается с одинаковой вероятностью, то такая стратегия называется абсолютно исследовательской.
\end{definition}
 
С точки зрения компромисса <<изучение-применение>> жадная стратегия -- это в чистом виде применение, без какой-либо возможности изучения. Абсолютно исследовательская  стратегия же скорее отвечает за постоянное изучение.
Рассмотрим применение жадной стратегии на следующем примере.

\begin{example}
Владелец небольшого магазина рассматривает возможности увеличения выручки. Он прочитал, что изменение расположения товара в магазине может стимулировать покупателей к импульсивным покупкам. Например, если расположить печенье рядом с чаем, то вероятность покупки печенья увеличивается, что увеличивает выручку. Владелец решил опробовать три различные конфигурации и понаблюдать за выручкой магазина.

Итак, определимся что есть что. Пусть <<ручка>> -- это одна из трех выбранных конфигураций ($a$, $b$, $c$), $t$ -- номер дня, в который проводится эксперимент, награда -- сумма потраченных покупателями денег в течение дня (выручка магазина). Пусть каждая игра состоит из $1000$ итераций (шагов) (по сути -- это количество дней, в течение которых проводится эксперимент). Награды в случае выбора конфигурации $a$, $b$ или $c$ -- это выборки из генеральных совокупностей $\xi_1$, $\xi_2$, $\xi_3$, имеющих распределения $\mathsf{N_{2,1}}$, $\mathsf{N_{2.5,1}}$ и $\mathsf{N_{3,1}}$, соответственно. Говоря формально, награды в рассматриваемом примере могут быть отрицательными, и могут трактоваться либо как ошибки в данных, либо как убытки магазина: утерянный, просроченный, разбитый товар, и  так далее.

Итак, как же поступает агент при использовании жадной стратегии? Пусть $q_0(a) = q_0(b) = q_0(c) = 0$. Так как в момент времени $t=0$ оценки ценности всех  действий (конфигураций) одинаковы, то выбор среди них производится случайным образом, а значит любая из трех конфигураций будет выбрана равновероятно. В итоге, стратегия на нулевом шаге такова:
$$
\pi_0(a|s_0) = \pi_0(b|s_0) = \pi_0(c|s_0) = \frac{1}{3}.
$$

Пусть, скажем, выбрана конфигурация $c$ (то есть $a_0 = c$). Тогда оценка ценности этой и только этой конфигурации станет положительной (предполагается, что выручка магазина в первый день все же положительна), а значит, в соответствии с алгоритмом, на первой итерации будет снова выбрана конфигурация $c$ (то есть $a_1 = c$), и так далее, а значит стратегия такова: 
$$
\pi_t(c|s_t) = 1, \quad t \geq 1.
$$
В итоге, в нашем примере (согласно жадной стратегии) будет всегда выбираться та конфигурация, которая была выбрана (случайным образом) на нулевом шаге. 

На рисунке \ref{Average_check_estimation} представлены результаты трех игр. Синяя кривая отвечает ситуации, когда в первой игре первым было выбрано действие $a_0 = c$ и, как мы только что отметили, оно же выбиралось на всех последующих шагах ($a_t = c$, $t \in \{0, 1, ..., 999\}$). Сам график показывает среднее значение награды или, что в нашем случае одно и то же, оценку ценности действия $c$ на каждом шаге. 

Во второй игре на первом шаге выбор пал на конфигурацию $a$, то есть $a_0 = a$ (красная кривая). График снова показывает величину средней награды при таком выборе начальной конфигурации (опять же, $a_t = a$, $t \in \{0, 1, ..., 999\}$). 

В третьей игре первым (как и последующими) было выбрано действие $b$. Значения средней награды в этом случае изображены оранжевым цветом. Видно, что максимально возможное среднее значение награды достигнуто только в одном из трех случаев. Кроме того, в одном из трех случаев достигнуто минимальное среднее значение награды.

<<Игры>>, описанные в данном примере, на практике могут быть реализованы следующим образом: есть три магазина некоторой сети, находящиеся в одинаковых условиях. Каждый из них независимо в течение $1000$ дней использует единственную конфигурацию и оценивает среднюю выручку. Далее происходит сравнение оценок и делается вывод о том, какая конфигурация работает лучше (если такая конфигурация имеется).
 \begin{figure}[h!]
	\center{\includegraphics[width=1\linewidth]{img/Average_check_estimation.png}}
	\caption{Средняя выручка в зависимости от выбранной конфигурации}
	\label{Average_check_estimation}
\end{figure}
\end{example}
Понятно, что выбор лучшей начальной конфигурации в нашем примере -- дело случая. Именно поэтому, с точки зрения оценки самой стратегии, нам интересно, как ведет себя алгоритм <<в среднем>>. Для этого проводят серию игр. На рисунке \ref{Average_check_estimation} черным цветом иллюстрируется среднее значение средней награды по трем играм на каждом шаге. На рисунке \ref{greedy} представлены результаты применения жадной стратегии при проведении $1000$ игр по $1000$ итераций в каждой. Черной линией отмечено среднее значение средней награды по всем играм. В отношении примера с конфигурациями магазина, описанный подход может быть реализован следующим образом: есть $1000$ магазинов некоторой сети, находящихся в одинаковых условиях. В каждом из магазинов проводится оценка одной из трех конфигураций в течение $1000$ дней. Тогда при использовании жадной стратегии, <<в среднем>> каждый магазин будет генерировать ежедневную выручку на уровне $2.5$ единиц.
 \begin{remark}
 	В дальнейшем, для сравнения стратегий мы будем также проводить $1000$ игр по $1000$ итераций в каждой, а результаты усреднять. Поскольку полученные результаты не будут являться результатами применения непосредственно стратегии, а будут являться усреднением по тысячекратному применению стратегии, то будем использовать термин алгоритм (например, жадный алгоритм).
 \end{remark}
 
\begin{figure}[h!]
	\center{\includegraphics[width=1\linewidth]{img/greedy.png}}
	\caption{Иллюстрация жадного алгоритма}
	\label{greedy}
\end{figure}
Конечно, существенным недостатком жадного алгоритма в рассмотренном примере является то, что исследование среды прекращается после первого же шага, что, конечно, серьезно сказывается на результатах. Как мы уже отмечали, на рисунке \ref{Average_check_estimation} видно, что <<жадный>> принцип в рассмотренном примере с магазинами только в одном из трех случаев приводит к максимальной возможной средней выручке. Более того, в одном из трех случаев он приводит даже к минимально возможной средней выручке. Причина этому -- запрет на какое-либо исследование. Исправить эту ситуацию помогает так называемая $\varepsilon$-жадная стратегия.