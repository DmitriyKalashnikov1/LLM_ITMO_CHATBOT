\documentclass[a4paper,14pt]{extarticle}
\usepackage[left = 20mm, right = 20mm, top = 25mm, bottom = 25mm]{geometry}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{euscript}
\usepackage{graphics}
\usepackage[T1, T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage{pgf,tikz}
\usepackage{multicol}
\usepackage{fancyvrb}
\usepackage{fvextra}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancyhdr} 
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[LE,RO]{Университет ИТМО}
\fancyhead[RE,LO]{Высшая школа цифровой культуры}
\fancyfoot[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{2pt}
 
% Добавляем свои блоки
 
\makeatletter
\algblock[ALGORITHMBLOCK]{BeginAlgorithm}{EndAlgorithm}
\algblock[BLOCK]{BeginBlock}{EndBlock}

\makeatother
 
% Нумерация блоков
 
\usepackage{caption}% http://ctan.org/pkg/caption
\captionsetup[ruled]{labelsep=period}
\renewcommand{\thealgorithm}{\arabic{algorithm}}

\sloppy

\usepackage{xcolor} % Required for specifying custom colours
\definecolor{grey}{rgb}{0.9,0.9,0.9} % Colour of the box surrounding the title
%\usepackage[sfdefault]{ClearSans} % Use the Clear Sans font (sans serif)
%\usepackage{XCharter} % Use the XCharter font (serif)
\definecolor{foo}{HTML}{EFF5F9}

\newcommand{\eqdef}{\overset{\mathrm{def}}{=\joinrel=}}
\def\re{{\rm Re}}
\def\im{{\rm Im}}
\def\dim{\rm dim}
\def\Ext{\rm Ext}
\def\wt#1{{{\widetilde #1} }}
\def\wh#1{{{\,\widehat #1\,} }}
\def\graph{{\rm gr\,}}
\def\ran{{\rm ran\,}}
\def\dom{{\rm dom\,}}
\def\ker{{\rm ker\,}}
\def\supp{{\rm supp\,}}
\def\diag{{\rm diag\,}}

\newcommand\dN{{\mathbb{N}}}
\newcommand\dR{{\mathbb{R}}}
\newcommand\dC{{\mathbb{C}}}
\newcommand{\bO}{{\mathbb{O}}}
\newcommand{\bU}{{\mathbb{U}}}
\newcommand\dZ{{\mathbb{Z}}}

\newcommand\gotB{{\mathfrak{B}}}
\newcommand\gotD{{\mathfrak{D}}}
\newcommand\gotH{{\mathfrak{H}}}
\newcommand\gotK{{\mathfrak{K}}}
\newcommand\gotL{{\mathfrak{L}}}
\newcommand\gotM{{\mathfrak{M}}}
\newcommand\gotN{{\mathfrak{N}}}
\newcommand\gotR{{\mathfrak{R}}}
\newcommand\gotS{{\mathfrak{S}}}
\newcommand\gotT{{\mathfrak{T}}}
\newcommand\gott{{\mathfrak{t}}}
\newcommand\gotC{{\mathfrak{C}}}
\newcommand\gotZ{{\mathfrak{Z}}}
\newcommand{\RNumb}[1]{\uppercase\expandafter{\romannumeral #1\relax}}


\newcommand{\ga}{{\alpha}}
\newcommand{\gd}{{\delta}}
\newcommand{\gD}{{\Delta}}
\newcommand{\gga}{{\gamma}}
\newcommand{\gG}{{\Gamma}}
\newcommand{\gF}{{\Phi}}
\newcommand{\gf}{{\phi}}
\newcommand{\gk}{{\kappa}}
\newcommand{\gK}{{\Kappa}}
\newcommand{\gl}{{\lambda}}
\newcommand{\gL}{{\Lambda}}
\newcommand{\gO}{{\Omega}}
\newcommand{\go}{{\omega}}
\newcommand{\gs}{{\sigma}}
\newcommand\gS{{\Sigma}}
\newcommand{\gT}{{\Theta}}
\newcommand{\gY}{{\Upsilon}}

\newcommand\cA{{\mathcal{A}}}
\newcommand\cB{{\mathcal{B}}}
\newcommand\cC{{\mathcal{C}}}
\newcommand\cD{{\mathcal{D}}}
\newcommand\cH{{\mathcal{H}}}
\newcommand\cK{{\mathcal{K}}}
\newcommand\cM{{\mathcal{M}}}
\newcommand\cN{{\mathcal{N}}}
\newcommand\cO{{\mathcal{O}}}
\newcommand\cP{{\mathcal{P}}}
\newcommand\cT{{\mathcal{T}}}
\newcommand\cU{{\mathcal{U}}}
\newcommand\cZ{{\mathcal{Z}}}

\newcommand\Rg{{\rm Rg}}
\newcommand\res{{\rm res}}
\newcommand\pr{{\rm Pr}}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Argmax}{Arg\,max}

\newtheorem{theorem}{Теорема}[subsection]
\newtheorem{lemma}{Лемма}[subsection]
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{definition}{Определение}[subsection]
\newtheorem{example}{Пример}[subsection]
\newtheorem{remark}{Замечание}[subsection]

\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}

\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\bead}{\begin{eqnarray*}}
\newcommand{\eead}{\end{eqnarray*}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\bed}{\begin{displaymath}}
\newcommand{\eed}{\end{displaymath}}

\newcommand{\bl}{\begin{lemma}}
\newcommand{\el}{\end{lemma}}

\newcommand{\bt}{\begin{theorem}}
\newcommand{\et}{\end{theorem}}

\newcommand{\bc}{\begin{corollary}}
\newcommand{\ec}{\end{corollary}}

\newcommand{\br}{\begin{remark}}
\newcommand{\er}{\end{remark}}

\newcommand{\bd}{\begin{definition}}
\newcommand{\ed}{\end{definition}}

\newcommand{\bspi}{\begin{split}}
\newcommand{\espi}{\end{split}}

\newcommand{\la}{\label}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}


\newenvironment{proof}%
{\begin{sloppypar}\noindent{\bf Доказательство.}}%
{\hspace*{\fill}$\square$\end{sloppypar}}
\renewcommand{\Large}{\fontsize{16}{25pt}\selectfont}

\newcommand{\slim}{\,\mbox{\rm s-}\hspace{-2pt} \lim}
\newcommand{\wlim}{\,\mbox{\rm w-}\hspace{-2pt} \lim}
\newcommand{\olim}{\,\mbox{\rm o-}\hspace{-2pt} \lim}
\newcommand{\transpose}[1]{\ensuremath{#1^{\scriptscriptstyle t}}}

\graphicspath{ {img/} }

\usepackage{indentfirst} 
\setlength\parindent{1cm}
% НАСТРОЙКИ ЗАГОЛОВКОВ
\usepackage{textcase} 
\usepackage{titlesec}
\titleformat{\section}[block]{\sffamily\Large\bfseries\filcenter}{\thesection}{0.5em}{}
\titleformat{\subsection}[block]{\large\sffamily\bfseries}{\thesubsection}{0.5em}{}
\titlespacing{\subsection}{2cm}{1mm}{3mm}

\begin{document}
%ДЛЯ АЛГОРИТМОВ
	% Перевод данных об алгоритмах
	\renewcommand{\listalgorithmname}{Список алгоритмов}
	\floatname{algorithm}{Алгоритм}
	
\algnewcommand{\Init}[1]{%
  \State \textbf{Инициализировать:}
  \Statex \hspace*{\algorithmicindent}\parbox[t]{.8\linewidth}{\raggedright #1}
}

	
	% Перевод команд псевдокода
	\algrenewcommand\algorithmicwhile{\textbf{Повторять пока }}
	\algrenewcommand\algorithmicdo{\textbf{Выполнять}}
	\algrenewcommand\algorithmicrepeat{\textbf{Повторять}}
	\algrenewcommand\algorithmicuntil{\textbf{Пока выполняется}}
	\algrenewcommand\algorithmicend{\textbf{Конец}}
	\algrenewcommand\algorithmicif{\textbf{Если}}
	\algrenewcommand\algorithmicelse{\textbf{иначе}}
	\algrenewcommand\algorithmicthen{\textbf{тогда}}
	\algrenewcommand\algorithmicfor{\textbf{Для}}
	\algrenewcommand\algorithmicforall{\textbf{Выполнить для всех}}
	\algrenewcommand\algorithmicfunction{\textbf{Функция}}
	\algrenewcommand\algorithmicprocedure{\textbf{Процедура}}
	\algrenewcommand\algorithmicloop{\textbf{повторять}}
	\algrenewcommand\algorithmicrequire{\textbf{Вход:}}
	\algrenewcommand\algorithmicensure{\textbf{Обеспечивающие условия:}}
	\algrenewcommand\algorithmicreturn{\textbf{Возвратить}}
	\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%
	\algrenewtext{EndWhile}{\textbf{Конец цикла}}
	\algrenewtext{EndLoop}{\textbf{Конец}}
	\algrenewtext{EndFor}{\textbf{конец цикла}}
	\algrenewtext{EndFunction}{\textbf{Конец функции}}
	\algrenewtext{EndProcedure}{\textbf{Конец процедуры}}
	\algrenewtext{EndIf}{\textbf{Конец условия}}
	\algrenewtext{EndFor}{\textbf{Конец цикла}}
	\algrenewtext{BeginAlgorithm}{\textbf{Начало алгоритма}}
	\algrenewtext{EndAlgorithm}{\textbf{Конец алгоритма}}
	\algrenewtext{BeginBlock}{\textbf{Начало блока. }}
	\algrenewtext{EndBlock}{\textbf{Конец блока}}
	\algrenewtext{ElsIf}{\textbf{иначе если }}
	
	\newcommand{\algorithmicbreak}{\textbf{Конец цикла}}
    \newcommand{\BREAK}{\STATE \algorithmicbreak}



%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	
	%------------------------------------------------
	%	Grey title box
	%------------------------------------------------
	\begin{center}
	\includegraphics[width=0.4\textwidth]{dc.png}
    \end{center}
    
    	\vfill
    	
	\colorbox{foo}{
		\parbox[t]{0.93\textwidth}{ % Outer full width box
			\parbox[t]{0.91\textwidth}{ % Inner box for inner right text margin
				\raggedleft % Right align the text
				%\fontsize{50pt}{80pt}\selectfont % Title font size, the first argument is the font size and the second is the line spacing, adjust depending on title length
				\vspace{0.7cm} % Space between the start of the title and the top of the grey box
				
				\huge Обучение с подкреплением\\
				
				\vspace{0.7cm} % Space between the end of the title and the bottom of the grey box
			}
		}
	}
	
	\vfill % Space between the title box and author information
	
	%------------------------------------------------
	%	Author name and information
	%------------------------------------------------

    

	
	\parbox[t]{0.93\textwidth}{ % Box to inset this section slightly
		\raggedleft % Right align the text
		%\large % Increase the font size
		{Высшая Школа Цифровой Культуры}\\[4pt] % Extra space after name
		Университет ИТМО\\[4pt] % Extra space before URL
		dc@itmo.ru\\[4pt]
		
		\hfill\rule{0.2\linewidth}{1pt}% Horizontal line, first argument width, second thickness
	}
	
\end{titlepage}

%----------------------------------------------------------------------------------------

\newpage
%
\pagestyle{empty}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\tableofcontents
%
\clearpage
\pagestyle{fancy}


\subsection{SARSA} 
Рассмотренные ранее методы обучения имеют достаточно серьезный недостаток. Дело в том, что они предполагают тот факт, что агент имеет представление о конфигурации среды. Иными словами, в уравнении Беллмана
$$
 v_{\pi}(s) = \sum\limits_{a \in A(s)}\pi(a|s)\sum\limits_{s^{\prime} \in S}\mathcal{P} _{ss^{\prime}}^a \left( \mathcal{R}_{ss^{\prime}}^a + \gamma v_{\pi}\left(s^{\prime}\right)\right),
$$
предполагаются известными и $\mathcal{P} _{ss^{\prime}}^a$, то есть вероятности перехода из состояния $s$ в состояние $s^{\prime}$ при совершении действия $a$, и все ожидаемые премии, то есть $\mathcal{R}_{ss^{\prime}}^a$. В реальности это почти всегда не так. Очевидный выход -- производить некоторые оценки на основе выполненных действий и полученных результатов. Идея очень похожа на идею, которая ранее была рассмотрена в разделе про многоруких бандитов. Вспомним рекуррентную формулу для нахождения ценности некоторого действия на основе экспоненциального скользящего среднего.
$$
q_{t + 1}(a) = q_{t}(a)  + \alpha \left(r_{t+1}-q_{t}(a)\right),
$$
где $\alpha \in (0,1]$.
Такой подход применим и для оценки ценности состояний. Он получил название Temporal Difference (TD). Учитывая, что  
$$
v_{\pi}(s) = \mathsf{E}_{\pi}\left(G_{t} | s_{t}=s\right)=\mathsf{E}_{\pi}\left(\sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1} | s_{t}=s\right) = \mathsf{E}_{\pi}\left(r_{t+1}+\gamma v_{\pi}\left(s^{\prime}\right)|s_t = s\right),
$$
можно получить способ оценки ценности состояния $s_t$:
$$
v(s_t) \leftarrow v(s_t)+\alpha(r_{t+1}+\gamma v(s_{t+1}) - v(s_t))
$$
В данном случае, находясь в состоянии $s_t$ агент, следуя какой-то стратегии, выполняет действие $a_t$, получает награду $r_{t+1}$ и переходит в состояние $s_{t+1}$. Ценность $v(s_{t+1})$ нового состояния $s_{t+1}$ также известна, а значит можно обновить ценность состояния $s_t$. Иными словами, на следующем шаге обновляется значение оценки ценности для предыдущего шага. 

Оценка ценности состояний хоть и важна, но не является самоцелью, так как награду приносят действия, а они выбираются в зависимости от стратегии. Поэтому при обучении нужно оптимизировать и стратегию. Для этих целей можно использовать идею TD для оценки функции действий $q(s, a)$, а именно
$$
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1}+\gamma q(s_{t+1},a_{t+1}) - q(s_t,a_t))
$$
Можно также заметить, что в процессе обучения повторяется одна и та же последовательность: состояние, действие, награда, состояние,.., то есть
$$
s,a,r,s,a,r,s,a,r,\ldots,
$$
поэтому такой метод и получил название $SARSA$. Зная оценку ценности каждого действия, можно использовать, например, жадную стратегию для выбора очередного действия, то есть стратегию, задаваемую следующим образом:
$$
A_{t} = \Argmax\limits_{a \in A(s_t)}  q_{t}(a), \quad 
\pi(a|s_t) = \begin{cases}
 \frac{1}{|A_t|}, & a \in A_t \\
 0, & \text{иначе}	
 \end{cases},
$$
но все же логично уделять некоторое внимание и исследованию. Для таких целей подходит, например, ранее рассмотренная $\varepsilon$-жадная стратегия:
$$
\pi_t(a|s_t) = \begin{cases}
	\displaystyle\frac{1-\varepsilon}{|A_t|} + \frac{\varepsilon}{|A(s_t)|}, & a\in A_t \\
	\\
	\displaystyle\frac{\varepsilon}{|A(s_t)|}, & a\not\in A_t
\end{cases},
$$
Ниже приведено формальное описание алгоритма $SARSA$.
\begin{algorithm}[H]
		\caption{Алгоритм SARSA}\label{ref:alg:EMB}
		\begin{algorithmic}[1]
			\State Пусть $S$ -- множество состояний, $A(s)$, $s \in S$ -- множество доступных действий в состоянии $s$. 
			\State Инициализировать $q(s,a)$, $s \in S$, $s$ -- нетерминальное, $a \in A(s)$ произвольно
			\State Инициализировать $\alpha$ и $\gamma$
			\For {\text{каждой игры повторять}}
			\State Инициализировать случайным образом нетерминальное состояние $s_0$
			\State Выбрать $a_0$ согласно стратегии $\pi_0(a|s_0)$
			\State $t \leftarrow 0$
			 \For{\text{каждого шага игры $t$}, пока не выполнен критерий остановки или пока $s_t$ -- не терминальное состояние,}
			\State Выполнить $a_t$, найти $r_{t+1}$, перейти в $s_{t+1}$
			\If{$s_{t+1}$ -- терминальное,}
			\State $
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1} - q(s_t,a_t))
$
			\Else
			\State Выбрать $a_{t+1}$ на основе стратегии $\pi_{t+1}(a|s_{t+1})$
			\State $
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1}+\gamma q(s_{t+1},a_{t+1}) - q(s_t,a_t))
$
			\EndIf
			\State $t \leftarrow (t+1)$
			\EndFor
			\EndFor
		\end{algorithmic}
	\end{algorithm}
Итак, согласно алгоритму, на первом этапе произвольно инициализируются ценности всех доступных действий $a \in A(s)$ для каждого нетерминального состояния $s \in S$. Далее проводится серия игр. В каждой игре начальное нетерминальное состояние агента инициализируется случайным образом. Далее, в рамках каждой игры на каждом шаге $t$ агент, находясь в состоянии $s_t$, выбирает некоторое действие $a_t$ согласно выбранной стратегии, переходит в следующее состояние $s_{t+1}$ и получает вознаграждение $r_{t+1}$. Если состояние $s_{t+1}$ -- не терминальное, то согласно стратегии выбирается действие $a_{t+1}$, а значение $q(s_t, a_t)$ обновляется согласно формуле.  
$$
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1}+\gamma q(s_{t+1},a_{t+1}) - q(s_t,a_t)).
$$
Далее происходит переход на новую итерацию игры.
Если состояние $s_{t+1}$ является терминальным, то $A(s_{t+1}) = \varnothing$ и
$$
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1}- q(s_t,a_t)).
$$
Игра завершается в случае, если совершен переход в терминальное состояние или выполнен критерий остановки. Примерами выполнения критерия остановки могут служить следующие ситуации:
\begin{itemize}
	\item достигнут заданный уровень суммы полученных наград;
	\item достигнут заданный уровень среднего значения полученных наград;
	\item достигнут заданный уровень числа потраченных шагов;
	\item и тому подобное.
\end{itemize}

Рассмотрим применение алгоритма $SARSA$ для уже знакомого, хотя и слегка модифицированного примера: будем считать состояние <<Cон>> терминальным (при переходе в него игра заканчивается). Схема представлена на рисунке \ref{example_6}.

\begin{figure}[h!]
	\center{\includegraphics[width=0.8\linewidth]{img/example_6.png}}
	\caption{Описание среды}
	\label{example_6}
\end{figure}


Положим $\gamma = 0.8$, $\alpha = 0.1$. В качестве стратегии будем использовать $\varepsilon$-жадную стратегию со значением $\varepsilon = 0.1$. Отметим также, что теперь нам неизвестны ни вероятности переходов из состояния $s$ в состояние $s^{\prime}$ в результате выполнения действия $a$, ни то, какие награды мы получим в этом случае. Все значения придется оценивать по ходу игры. Для удобства сформируем таблицу ценностей действий в зависимости от состояний и произведем начальную инициализацию (установим все возможные значения ценностей равными нулю). Обращаем внимание, что символ $\times$ означает, что действие не может быть выбрано в текущем состоянии.

\begin{center}
\begin{tabular}{ || c | c | c |c | c || }
\hline
 & \textbf{$a_1$} & \textbf{$a_2$} & \textbf{$a_3$} & \textbf{$a_4$} \\ \hline
$s_1$ & $ 0 $ & $0 $ & $\times $ & $\times $ \\\hline
$s_2$ & $0$ & $\times$ & $\times$ & $0$\\ \hline
$s_3$ & $\times$ & $0$ & $0$ & $\times$ \\ \hline
$s_4$ & $\times$ & $0$ & $\times$ & $0$ \\ \hline
$s_5$ & $\times$ & $\times$ & $\times$ & $\times$ \\ \hline
\end{tabular}
\end{center}

Предположим, что мы начинаем в состоянии $s_1$. В нем нам доступны действия $a_1$ и $a_2$, причем на данный момент $q(s_1,a_1) = q(s_1,a_2) = 0$. Согласно нашей стратегии, в данном случае произвольным образом выбирается любое из доступных действий. Пусть выбрано действие $a_2$. Тогда агент переходит в состояние $s_2$ и получает награду $r=-2$. Далее, находясь в состоянии $s_2$, нужно выбрать следующее действие. Нам доступны $a_1$ и $a_4$. Так как ценности одинаковые (равны нулю), выбираем любое, например $a_4$. Теперь можно обновить $q(s_1, a_2)$. 

$$
q(s_1,a_2) \leftarrow q(s_1, a_2)+\alpha(r +\gamma q(s_{2},a_{4}) - q(s_1,a_2)) = 
$$
$$
= 0  +  0.1  \cdot  ( -2  +  0.8  \cdot  0  -  0 ) = -0.2.
$$
Занесем полученное значение в таблицу.
\begin{center}
\begin{tabular}{ || c | c | c |c | c || }
\hline
 & \textbf{$a_1$} & \textbf{$a_2$} & \textbf{$a_3$} & \textbf{$a_4$} \\ \hline
$s_1$ & $ 0 $ & $-0.2 $ & $\times $ & $\times $ \\\hline
$s_2$ & $0$ & $\times$ & $\times$ & $0$\\ \hline
$s_3$ & $\times$ & $0$ & $0$ & $\times$ \\ \hline
$s_4$ & $\times$ & $0$ & $\times$ & $0$ \\ \hline
$s_5$ & $\times$ & $\times$ & $\times$ & $\times$ \\ \hline
\end{tabular}
\end{center}
Продолжим выполнение алгоритма. \\
Текущее состояние: $s_2$ \\
Выбранное действие: $a_4$ \\
Следующее состояние: $s_5$\\
Награда: $7$ \\
Отметим, что так как $s_5$ -- терминальное состояние, то игра заканчивается. Тогда
$$
q(s_2,a_4) \leftarrow  0  +  0.1  \cdot  ( 7   -  0 ) = 0.7.
$$
\begin{center}
\begin{tabular}{ || c | c | c |c | c || }
\hline
 & \textbf{$a_1$} & \textbf{$a_2$} & \textbf{$a_3$} & \textbf{$a_4$} \\ \hline
$s_1$ & $ 0 $ & $-0.2 $ & $\times $ & $\times $ \\\hline
$s_2$ & $0$ & $\times$ & $\times$ & $0.7$\\ \hline
$s_3$ & $\times$ & $0$ & $0$ & $\times$ \\ \hline
$s_4$ & $\times$ & $0$ & $\times$ & $0$ \\ \hline
$s_5$ & $\times$ & $\times$ & $\times$ & $\times$ \\ \hline
\end{tabular}
\end{center}
Произведя еще несколько игр, получим следующую таблицу.
\begin{center}
\begin{tabular}{ || c | c | c |c | c || }
\hline
 & \textbf{$a_1$} & \textbf{$a_2$} & \textbf{$a_3$} & \textbf{$a_4$} \\ \hline
$s_1$ & $ -0.32 $ & $0.88 $ & $\times $ & $\times $ \\\hline
$s_2$ & $-0.02$ & $\times$ & $\times$ & $6.5$\\ \hline
$s_3$ & $\times$ & $-0.09$ & $-0.19$ & $\times$ \\ \hline
$s_4$ & $\times$ & $0.99$ & $\times$ & $6.14$ \\ \hline
$s_5$ & $\times$ & $\times$ & $\times$ & $\times$ \\ \hline
\end{tabular}
\end{center}
Итак, для оценки выполненного действия мы регулярно как бы заглядываем в будущее и заранее выбираем следующее действие, что, в конечном счете, сказывается на оценке текущего действия.
\begin{remark}
	Заметим, что на основе последней таблицы можно сформировать оптимальную стратегию (если выбирать действия жадно), которая немного отличается от найденной теоретической из-за терминальности состояния $s_5$ (рисунок \ref{example_7}). Находясь в состоянии $s_1$, надо выбирать действие $a_2$, так как 
	
	$$
	q(s_1, a_2) = 0.88 > q(s_1, a_1) = -0.32.
	$$
Находясь в состоянии $s_2$ -- выбирать действие $a_4$, так как
	$$
	q(s_2, a_4) = 6.5 > q(s_2, a_1) = -0.02
	$$
и так далее.
\begin{figure}[h!]
	\center{\includegraphics[width=0.8\linewidth]{img/example_7.png}}
	\caption{Оптимальная стратегия}
	\label{example_7}
\end{figure}

\end{remark}
\begin{remark}
	В нашем примере получаемые в процессе игры награды имеют одни и те же значения вне зависимости от $s$, $s^{\prime}$ и $a$. В более общем случае это может быть не так.
\end{remark} 

 

\subsection{Q-обучение}

Рассмотрим еще один алгоритм обучения на основе TD. Ранее нами были получены уравнения оптимальности Беллмана, в частности уравнение для оптимальной функции ценности действий
$$
q^{\ast}(s,a) = \sum\limits_{s^{\prime} \in S}\mathcal{P}_{ss^{\prime}}^a \left( \mathcal{R}_{ss^{\prime}}^a + \gamma \underset{a^{\prime} \in A(s')}{\max} \ q^{\ast}(s^{\prime},a^{\prime}) \right).
$$ 

Идея метода $Q$-обучения заключается в аппроксимации оптимальной функции ценности следующим образом

$$
q(s_t, a_t) \leftarrow q(s_t, a_t) + \left( r_{t+1} + \gamma \underset{a \in A(s_{t+1})}{\max}  \ q(s_{t+1}, a) - q(s_t, a_t) \right).
$$  
Стоит на секунду задуматься: а в чем отличие $SARSA$ и $Q$-обучения? Дело в том, что для обновления $q(s_t, a_t)$ в $SARSA$ используется значение $q(s_{t+1},a_{t+1})$, то есть ценность действия $a_{t+1}$, выбранного в состоянии $s_{t+1}$ согласно некоторой установленной стратегии $\pi_{t + 1}(a|s_{t+1})$. В $Q$-обучении используется оптимальная стратегия, так как выбирается то действие, для которого достигается $\underset{a \in A(s_{t+1})}{\max} q(s_{t+1}, a)$. В итоге при обновлении значения функции ценности действия мы опираемся не на выбранную стратегию (возможно, не оптимальную), а на оптимальную, но на следующем шаге (!). Если текущая стратегия оптимальна, то алгоритмы работают одинаково. Запишем алгоритм $Q$-обучения формально.

\begin{algorithm}[H]
		\caption{Алгоритм Q-обучения}\label{ref:alg:EMB}
		\begin{algorithmic}[1]
			\State Пусть $S$ -- множество состояний, $A(s)$, $s \in S$ -- множество доступных действий в состоянии $s$. 
			\State Инициализировать $q(s,a)$, $s \in S$, $s$ -- нетерминальное, $a \in A(s)$ произвольно
			\State Инициализировать $\alpha$ и $\gamma$
			\For {\text{каждой игры повторять}}
			\State Инициализировать случайным образом нетерминальное состояние $s_0$
			\State $t \leftarrow 0$
			 \For{\text{каждого шага игры $t$}, пока не выполнен критерий остановки или пока $s_t$ -- не терминальное состояние,}
			 \State Выбрать $a_t$ согласно стратегии $\pi_t(a|s_t)$
			\State Выполнить $a_t$, найти $r_{t+1}$, перейти в $s_{t+1}$
			\If{$s_{t+1}$ -- терминальное,}
			\State $
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1} - q(s_t,a_t))
$
			\Else
			\State $
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1}+\gamma \underset{a \in A(s_{t+1})}{\max}  \ q(s_{t+1}, a) - q(s_t,a_t))
$
			\EndIf		
			\State $t \leftarrow (t+1)$
			\EndFor
			\EndFor
		\end{algorithmic}
	\end{algorithm}
Во-первых, как мы уже отмечали ранее, разница с $SARSA$ заключается в способе оценки ценности действия. Во-вторых отличается выбор действия: в $SARSA$ на текущей итерации находилось как текущее так и следующее действие, в то время как в $Q$-обучении происходит выбор только текущего действия. 
Если состояние $s_{t+1}$ -- не терминальное, то $q(s_t, a_t)$ обновляется согласно формуле  
$$
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1}+\gamma \underset{a \in A(s_{t+1})}{\max}  \ q(s_{t+1}, a) - q(s_t,a_t)).
$$
Далее происходит переход на новую итерацию игры. Если состояние $s_{t+1}$ является терминальным, то $A(s_{t+1}) = \varnothing$ и
$$
q(s_t,a_t) \leftarrow q(s_t, a_t)+\alpha(r_{t+1}- q(s_t,a_t)).
$$
Игра завершается в случае, если выполнен критерий остановки, или совершен переход в терминальное состояние. 
  
Поясним отличия алгоритма $Q$-обучения и алгоритма $SARSA$ на все том же примере. Пусть на каком-то этапе обучения агент окажется в состоянии $s_1$, выберет действие $a_2$, а таблица оценок будет иметь вид
\begin{center}
\begin{tabular}{ || c | c | c |c | c || }
\hline
 & \textbf{$a_1$} & \textbf{$a_2$} & \textbf{$a_3$} & \textbf{$a_4$} \\ \hline
$s_1$ & $ -0.32 $ & $0.88 $ & $\times $ & $\times $ \\\hline
$s_2$ & $-0.02$ & $\times$ & $\times$ & $6.5$\\ \hline
$s_3$ & $\times$ & $-0.09$ & $-0.19$ & $\times$ \\ \hline
$s_4$ & $\times$ & $0.99$ & $\times$ & $6.14$ \\ \hline
$s_5$ & $\times$ & $\times$ & $\times$ & $\times$ \\ \hline
\end{tabular}
\end{center}
Тогда уточнение оценки $q(s_1, a_2)$ вне зависимости от используемой стратегии будет произведено следующим образом: 
$$
q(s_1,a_2) \leftarrow  0.88  +  0.1  \cdot  ( -2  +  0.8  \cdot  6.5  -  0.88 ) \approx 1.12,
$$
так как 
$$
q(s_2,a_4) = \max (q(s_2,a_1), q(s_2,a_4)).
$$
В алгоритме $SARSA$ это не всегда так. Например, если бы в результате использования $\varepsilon$-жадной стратегии в состоянии $s_2$ было выбрано действие не $a_4$, а $a_1$, то для обновления ценности $q(s_1,a_2)$ использовалось бы значение $q(s_2,a_1)$. 
В случае $Q$-обучения стратегия (например $\varepsilon$-жадная) используется только при выборе текущего действия и не участвует в обновлении оценки ценности. 
\subsection{Пример}
Рассмотрим примеры использования рассмотренных алгоритмов. Симулируем игру, в которой агенту нужно найти выход из лабиринта, пример возможного решения представлен на рисунке \ref{maze}. 
\begin{figure}[h!]
	\center{\includegraphics[width=0.6\linewidth]{img/maze.png}}
	\caption{Пример выхода из лабиринта.}
	\label{maze}
\end{figure}

В процессе игры агент помещается в точку старта, ему доступны перемещения в четырех направлениях (вверх, вправо, вниз и влево). Цель агента -- достичь точки выхода из лабиринта. В результате каждого действия агент получает награду. Награды суммируются в процессе игры. Каждый шаг сопровождается наградой $-0.05$. Если агент переместился в уже посещенную ячейку, награда составляет $-0.25$. Если в результате действия агент упирается в стену, награда составляет $-0.75$. Выход из лабиринта сопровождается наградой $+10$. В рамках одной игры терминальное состояние достигается всегда: либо игрок находит выход и тогда игра завершается победой, либо сумма полученных наград окажется меньше заданного порогового значения. В описываемом случае пороговое значение -- это 
$$
-\frac{1}{2}\cdot(\text{Размер лабиринта}) = -32.
$$
Для обучения использовался $\varepsilon$-жадный алгоритм со значением $\varepsilon = 0.1$. Коэффициент дисконтирования $\gamma$  равен $0.9$, коэффициент $\alpha = 0.1$. 

%\begin{figure}[h]
%\begin{minipage}[h]{0.49\linewidth}
%\center{\includegraphics[width=1\linewidth]{sarsa_param.png} \\ а)}
%\end{minipage}
%\hfill
%\begin{minipage}[h]{0.49\linewidth}
%\center{\includegraphics[width=1\linewidth]{q_param.png} \\ б)}
%\end{minipage}
%\caption{Сравнение алгоритмов обучения.}
%\label{param}
%\end{figure}

Агенту доступно $44$ начальных состояния (белые клетки за исключением клетки выхода из лабиринта). Эпизодом будем называть серию из $44$ игр, где каждое состояние (клетка) используется ровно один раз в качестве начального. Обучение заканчивается, если агент умеет находить путь к выходу из любого из $44$ доступных состояний (не обязательно оптимальный). Иными словами, оказавшись в любом состоянии, агент точно знает какие действия нужно совершать, чтобы гарантированно выйти из лабиринта. 

%Алгоритму $Q$-обучения потребовалось всего $76$ эпизодов по $44$ игры в каждом против $188$ эпизодов у $SARSA$. Кумулятивной наградой будем называть сумму полученных наград. Соответствующие графики представлены на рисунках \ref{param}.a) и  \ref{param}.б) для $SARSA$ и $Q$-обучения, соответственно. Для <<выхода в плюс>> с точки зрения кумулятивной награды каждому алгоритму понадобилось порядка $60$ эпизодов.  Кроме того, на рисунках представлены графики доли выигранных игр для каждого эпизода. Под долей выигранных игр понимается отношение количества побед в рамках эпизода к сорока четырем. Отметим, что алгоритм $Q$-обучения в данном случае продемонстрировал более плавное увеличение доли выигранных игр.

На основе полученных значений ценностей действий для всех состояний можно составить оптимальную стратегию. Полученные стратегии для алгоритмов $SARSA$ и $Q$-обучения представлены на рисунках \ref{policy}.a) и  \ref{policy}.б), соответственно.

\begin{figure}[h]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=1\linewidth]{sarsa_policy.png} \\ а)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.98\linewidth]{q_policy.png} \\ б)}
\end{minipage}
\caption{Стратегии алгоритмов $SARSA$ и $Q$-обучения.}
\label{policy}
\end{figure}
Можно заметить некоторые интересные различия, связанные с использованием $\varepsilon$-жадных стратегий.  Посмотрите на состояние $(3,3)$ (клетка, отвечающая пересечению третьей строки и третьего столбца). Если начинать в этом состоянии, то стратегия, полученная согласно методу $SARSA$, предлагает следовать вниз, и тогда выход будет найден за $17$ шагов. Стратегия, полученная на основе $Q$-обучения, наоборот говорит идти вверх, но тогда выход будет найден через $21$ шаг. Для этого состояния $SARSA$ оказался более эффективным. С другой стороны, у $SARSA$ можно заметить некоторую странность в ячейке $(6,3)$ (пересечение $6$ строки и $3$ столбца). Различия, например, как в ячейке $(5, 8)$, роли не играют.

\subsection{Резюме}
В заключение отметим, что обучение с подкреплением -- это, возможно, одно из самых интересных и многообещающих направлений машинного обучения. Такой тип обучения сочетает в себе как обучение с учителем, так и обучение без учителя. С одной стороны, человеческого вмешательства не требуется, агент обучается самостоятельно. С другой стороны, учителем в некотором смысле выступает среда. Еще одним существенным плюсом является то, что обучение можно производить в динамично меняющейся среде. Отметим, что в этой лекции мы рассмотрели лишь небольшую часть методов обучения с подкреплением, которых на самом деле достаточно много. При этом новые решения и идеи в этой области публикуются с завидной регулярностью. 
