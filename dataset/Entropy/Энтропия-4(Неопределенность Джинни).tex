\documentclass[a4paper,14pt]{extarticle}


\usepackage[left = 20mm, right = 20mm, top = 25mm, bottom = 25mm, headheight=30pt]{geometry}
\usepackage{amssymb}
\usepackage{cmap}
\usepackage{enumerate}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{euscript}
\usepackage{graphics}
\usepackage[T1, T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage{pgf,tikz}
\usepackage{multicol}
\usepackage{float}
\restylefloat{table}
\usepackage{multirow}
\usepackage{caption}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancyhdr} 
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[LE,RO]{Университет ИТМО}
\fancyhead[RE,LO]{Высшая школа цифровой культуры}
\fancyfoot[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{2pt}


\sloppy



\usepackage{xcolor} % Required for specifying custom colours
\definecolor{grey}{rgb}{0.9,0.9,0.9} % Colour of the box surrounding the title
%\usepackage[sfdefault]{ClearSans} % Use the Clear Sans font (sans serif)
%\usepackage{XCharter} % Use the XCharter font (serif)
\definecolor{foo}{HTML}{EFF5F9}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Argmin}{Arg\,min}
\DeclareMathOperator*{\Argmax}{Arg\,max}
\DeclareMathOperator*{\logloss}{logloss}
\newcommand{\eqdef}{\overset{\mathrm{def}}{=\joinrel=}}
\def\re{{\rm Re}}
\def\im{{\rm Im}}
\def\dim{\rm dim}
\def\Ext{\rm Ext}
\def\wt#1{{{\widetilde #1} }}
\def\wh#1{{{\,\widehat #1\,} }}
\def\graph{{\rm gr\,}}
\def\ran{{\rm ran\,}}
\def\dom{{\rm dom\,}}
\def\ker{{\rm ker\,}}
\def\supp{{\rm supp\,}}
\def\diag{{\rm diag\,}}

\newcommand\dN{{\mathbb{N}}}
\newcommand\dR{{\mathbb{R}}}
\newcommand\dC{{\mathbb{C}}}
\newcommand{\bO}{{\mathbb{O}}}
\newcommand{\bU}{{\mathbb{U}}}
\newcommand\dZ{{\mathbb{Z}}}

\newcommand\gotB{{\mathfrak{B}}}
\newcommand\gotD{{\mathfrak{D}}}
\newcommand\gotH{{\mathfrak{H}}}
\newcommand\gotK{{\mathfrak{K}}}
\newcommand\gotL{{\mathfrak{L}}}
\newcommand\gotM{{\mathfrak{M}}}
\newcommand\gotN{{\mathfrak{N}}}
\newcommand\gotR{{\mathfrak{R}}}
\newcommand\gotS{{\mathfrak{S}}}
\newcommand\gotT{{\mathfrak{T}}}
\newcommand\gott{{\mathfrak{t}}}
\newcommand\gotC{{\mathfrak{C}}}
\newcommand\gotZ{{\mathfrak{Z}}}
\newcommand{\RNumb}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\newcommand{\ga}{{\alpha}}
\newcommand{\gd}{{\delta}}
\newcommand{\gD}{{\Delta}}
\newcommand{\gga}{{\gamma}}
\newcommand{\gG}{{\Gamma}}
\newcommand{\gF}{{\Phi}}
\newcommand{\gf}{{\phi}}
\newcommand{\gk}{{\kappa}}
\newcommand{\gK}{{\Kappa}}
\newcommand{\gl}{{\lambda}}
\newcommand{\gL}{{\Lambda}}
\newcommand{\gO}{{\Omega}}
\newcommand{\go}{{\omega}}
\newcommand{\gs}{{\sigma}}
\newcommand\gS{{\Sigma}}
\newcommand{\gT}{{\Theta}}
\newcommand{\gY}{{\Upsilon}}


\newcommand\cA{{\mathcal{A}}}
\newcommand\cB{{\mathcal{B}}}
\newcommand\cC{{\mathcal{C}}}
\newcommand\cD{{\mathcal{D}}}
\newcommand\cH{{\mathcal{H}}}
\newcommand\cK{{\mathcal{K}}}
\newcommand\cM{{\mathcal{M}}}
\newcommand\cN{{\mathcal{N}}}
\newcommand\cO{{\mathcal{O}}}
\newcommand\cP{{\mathcal{P}}}
\newcommand\cT{{\mathcal{T}}}
\newcommand\cU{{\mathcal{U}}}
\newcommand\cZ{{\mathcal{Z}}}

\newcommand\Rg{{\rm Rg}}
\newcommand\res{{\rm res}}
\newcommand\pr{{\rm Pr}}

\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}[subsection]
\newtheorem{lemma}{Лемма}[subsection]
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{definition}{Определение}[subsection]
\newtheorem{example}{Пример}[subsection]
\newtheorem{remark}{Замечание}[subsection]

\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}

\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\bead}{\begin{eqnarray*}}
\newcommand{\eead}{\end{eqnarray*}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\bed}{\begin{displaymath}}
\newcommand{\eed}{\end{displaymath}}

\newcommand{\bl}{\begin{lemma}}
\newcommand{\el}{\end{lemma}}

\newcommand{\bt}{\begin{theorem}}
\newcommand{\et}{\end{theorem}}

\newcommand{\bc}{\begin{corollary}}
\newcommand{\ec}{\end{corollary}}

\newcommand{\br}{\begin{remark}}
\newcommand{\er}{\end{remark}}

\newcommand{\bd}{\begin{definition}}
\newcommand{\ed}{\end{definition}}

\newcommand{\bspi}{\begin{split}}
\newcommand{\espi}{\end{split}}

\newcommand{\la}{\label}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}


\newenvironment{proof}%
{\begin{sloppypar}\noindent{\bf Доказательство.}}%
{\hspace*{\fill}$\square$\end{sloppypar}}
\renewcommand{\Large}{\fontsize{16}{25pt}\selectfont}

\newcommand{\slim}{\,\mbox{\rm s-}\hspace{-2pt} \lim}
\newcommand{\wlim}{\,\mbox{\rm w-}\hspace{-2pt} \lim}
\newcommand{\olim}{\,\mbox{\rm o-}\hspace{-2pt} \lim}
\newcommand{\transpose}[1]{\ensuremath{#1^{\scriptscriptstyle t}}}




\graphicspath{ {img/} }

\usepackage{indentfirst} 
\setlength\parindent{1cm}

\usepackage{textcase} 
\usepackage{titlesec}

\usepackage{indentfirst} 
\setlength\parindent{1cm}
% НАСТРОЙКИ ЗАГОЛОВКОВ
\usepackage{textcase} 
\usepackage{titlesec}
\titleformat{\section}[block]{\sffamily\Large\bfseries\filcenter}{\thesection}{0.5em}{}
\titleformat{\subsection}[block]{\large\sffamily\bfseries}{\thesubsection}{0.5em}{}
\titlespacing{\subsection}{2cm}{1mm}{3mm}

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	
	%------------------------------------------------
	%	Grey title box
	%------------------------------------------------
	\begin{center}
	\includegraphics[width=0.4\textwidth]{dc.png}
    \end{center}
    
    	\vfill
    	
	\colorbox{foo}{
		\parbox[t]{0.93\textwidth}{ % Outer full width box
			\parbox[t]{0.91\textwidth}{ % Inner box for inner right text margin
				\raggedleft % Right align the text
				%\fontsize{50pt}{80pt}\selectfont % Title font size, the first argument is the font size and the second is the line spacing, adjust depending on title length
				\vspace{0.7cm} % Space between the start of the title and the top of the grey box
				
				\huge Энтропия\\ Деревья принятия решений
				
				\vspace{0.7cm} % Space between the end of the title and the bottom of the grey box
			}
		}
	}
	
	\vfill % Space between the title box and author information
	
	%------------------------------------------------
	%	Author name and information
	%------------------------------------------------

    

	
	\parbox[t]{0.93\textwidth}{ % Box to inset this section slightly
		\raggedleft % Right align the text
		%\large % Increase the font size
		{Высшая Школа Цифровой Культуры}\\[4pt] % Extra space after name
		Университет ИТМО\\[4pt] % Extra space before URL
		dc@itmo.ru\\[4pt]
		
		\hfill\rule{0.2\linewidth}{1pt}% Horizontal line, first argument width, second thickness
	}
	
\end{titlepage}

%----------------------------------------------------------------------------------------


\newpage
%
\pagestyle{empty}
\tableofcontents
%
\clearpage
\pagestyle{fancy}


\section{Неопределенность Джини}
\subsection{Определение и свойства}
При построении деревьев принятия решений используют и другие критерии информативности.   Среди них можно выделить особо так называемую неопределенность Джини (Gini impurity), которая является мерой неправильной классификации.  

Что значит неправильная классификация? Для демонстрации происходящего, снова вернемся к эксперименту с опросом друзей. В нем мы получили, что из $14$ опрошенных друзей $9$ согласились пойти играть в футбол, а $5$ нет. Тогда эксперимент задается следующей таблицей
\begin{center}
\begin{tabular}{c|c|c}
$\Omega$ & Да & Нет \\
\hline $\mathsf P$ & $\frac{9}{14}$ & $\frac{5}{14}$
\end{tabular}.
\end{center}
Предположим, что мы берем случайного друга. Какова вероятность события, что мы его неверно классифицируем? Это событие распадается на два: он либо хочет играть в футбол, а мы его отнесем к группе, которая не хочет, либо наоборот. Ясно, что вероятность такого события может быть вычислена, как
$$
\mathsf P(\text{друг неверно классифицирован}) = \frac{9}{14}\cdot \frac{5}{14} + \frac{5}{14} \cdot \frac{9}{14} = \frac{90}{196}.
$$
Эта величина и есть неопределенность Джини или мера неправильной классификации.
Итак, введем формальное определение.
\begin{definition}
Пусть эксперимент $\Omega$ описывается таблицей 
\begin{center}
\begin{tabular}{c|c|c|c|c}
$\Omega$ & $\omega_1$ &  $\omega_2$ & $\ldots$ & $\omega_n$ \\
\hline $\mathsf P$ & $\mathsf{P}_1$ & $\mathsf{P}_2$ & $\ldots$ & $\mathsf{P}_n$
\end{tabular}.
\end{center}
Неопределенностью Джини $\mathsf G(\Omega)$ эксперимента $\Omega$ называется величина
$$
\mathsf{G}(\Omega) = \sum \limits_{i=1}^n\mathsf{P}_i (1-\mathsf{P}_i).
$$
\end{definition}
\begin{remark}
Полезно заметить, что выражения для неопределенности Джини эквивалентным образом может быть переписано и вот так:
$$
\mathsf{G}(\Omega) = 1 - \sum \limits_{i=1}^n  \mathsf{P}^2_i.
$$
Это представление легко получается из следующих выкладок:
$$
\mathsf{G}(\Omega) = \sum \limits_{i=1}^n \mathsf{P}_i (1-\mathsf{P}_i)  = \sum \limits_{i=1}^n \left ( \mathsf{P}_i -\mathsf{P}^2_i \right ) = \sum \limits_{i=1}^n  \mathsf{P}_i - \sum \limits_{i=1}^n  \mathsf{P}^2_i = 1 - \sum \limits_{i=1}^n  \mathsf{P}^2_i.
$$
\end{remark}
Как же связаны неопределенность Джини и энтропия? Оказывается, неопределенность Джини обладает схожими свойствами, что и энтропия.
\begin{theorem}
Пусть рассматривается эксперимент $\Omega$. Тогда
$$
\mathsf G(\Omega) = 0 \Leftrightarrow \mathsf H(\Omega) = 0.
$$
\end{theorem}
Так как энтропия равна нулю тогда и только тогда, когда у эксперимента ровно один исход имеет вероятность $1$ (то есть нет неопределенности), то то же самое можно сказать и про неопределенность Джини.
\begin{proof}
Если $\mathsf H(\Omega) = 0$, то существует исход эксперимента $\omega_i$ такой, что его вероятность $\mathsf P_i = 1$. Тогда вероятности всех остальных исходов (если они есть) равны $0$, а значит
$$
\mathsf G(\Omega) = 1 - \mathsf P_i^2 = 1 - 1 = 0.
$$
Обратно, если $\mathsf G(\Omega) = 0$, то 
$$
\sum\limits_{i = 1}^n \mathsf P_i (1 - \mathsf P_i) = 0.
$$
Так как все слагаемые неотрицательны, то каждое из них равно $0$, а значит $\mathsf P_i(1 - \mathsf P_i) = 0$ при всех $i \in \{1, 2, ..., n\}$. В итоге, для каждого $\mathsf P_i$ выполнено одно из двух: оно либо равно $0$, либо $1$. Но так как сумма всех $\mathsf P_i $ равна $1$, то лишь одно $\mathsf P_i = 1$, а остальные равны нулю. Значит, и энтропия такого эксперимента равна нулю.
\end{proof}
Аналогично энтропии, неопределенность Джини максимальна в случае, когда все исходы эксперимента $\Omega$ равновозможны, а именно справедливо следующее наблюдение.
\begin{theorem}
Неопределенность Джини $\mathsf G$ максимальна в случае, когда все исходы эксперимента равновозможны, то есть, когда эксперимент описывается таблицей вида
\begin{center}
\begin{tabular}{c|c|c|c|c}
$\Omega$ & $\omega_1$ &  $\omega_2$ & $\ldots$ & $\omega_n$ \\
\hline $\mathsf P$ & $\frac{1}{n}$ & $\frac{1}{n}$ & $\ldots$ & $\frac{1}{n}$
\end{tabular}
\end{center}
В этом случае неопределенность Джини равна
$$
\mathsf G (\Omega) = 1 - \frac{1}{n},
$$
\end{theorem}
\begin{proof}
Ясно, что на эксперименте, заданным таблицей
\begin{center}
\begin{tabular}{c|c|c|c|c}
$\Omega$ & $\omega_1$ &  $\omega_2$ & $\ldots$ & $\omega_n$ \\
\hline $\mathsf P$ & $\frac{1}{n}$ & $\frac{1}{n}$ & $\ldots$ & $\frac{1}{n}$
\end{tabular}
\end{center}
получаем, что
$$
\mathsf G(\Omega) = 1 - \sum\limits_{i = 1}^n \frac{1}{n^2} = 1 - \frac{1}{n}.
$$
Докажем, что это значение максимально. Так как функция $\frac{1}{x}$ выпукла вниз, то для нее справедливо неравенство Йенсена: для любых чисел $p_1, p_2, ..., p_n > 0$ таких, что $p_1 + p_2 + ... + p_n = 1$ и любых $x_1, x_2, ..., x_n$из интервала выпуклости функции справедливо:
$$
f(p_1x_1 + p_2x_2 + ... + p_n x_n) \leq p_1 f(x_1) + p_2 f(x_2) + ... + p_n f(x_n)
$$
или
$$
f\left(\sum\limits_{i = 1}^n p_ix_i \right) \leq \sum\limits_{i = 1}^n p_i f(x_i).
$$
Положим $p_i = \mathsf P_i$ и $x_i = \frac{1}{\mathsf P_i}$, получим
$$
f\left(\mathsf P_1 \cdot  \frac{1}{\mathsf P_1} + ... + \mathsf P_n \cdot  \frac{1}{\mathsf P_n} \right) = f(n) = \frac{1}{n} \leq \sum\limits_{i = 1}^n \mathsf P_i \frac{1}{\frac{1}{\mathsf P_i}} = \sum\limits_{i = 1}^n \mathsf P_i^2,
$$
откуда
$$
1 - \sum\limits_{i = 1}^n \mathsf P_i^2 \leq 1 - \frac{1}{n}.
$$
\end{proof}
Легко видеть, что, как и энтропия, неопределенность Джини растет с ростом количества равновозможных исходов эксперимента $\Omega$.
\begin{corollary}
Из последней теоремы легко получить важное следствие: неопределенность Джини $\mathsf G(\Omega)$ любого эксперимента $\Omega$ всегда находится в следующих пределах:
$$
0 \leq \mathsf G(\Omega) < 1.
$$	
\end{corollary}

Как и ранее, резонно задаться вопросом: а как определяется неопределенность Джини в случае эксперимента $(\Omega, \Theta)$?
Напомним, что экспериментом $(\Omega, \Theta)$ называется произвольное множество пар исходов $(\omega_i, \theta_j)$, $i \in \{1, 2, \ldots, m\}$, $j \in \{1, 2, \ldots, n\}$, каждой из которых сопоставлено число $\mathsf P_{ij} \geq 0$, называемое вероятностью исхода $(\omega_i, \theta_j)$ такое, что
$$
\sum\limits_{i = 1}^m \sum\limits_{j = 1}^n \mathsf P_{ij} = 1.
$$
При этом эксперимент $(\Omega, \Theta)$ может быть описан (и отождествлен)  следующей таблицей:
\begin{center}
\begin{tabular}{c|c|c|c|c}
$(\Omega, \Theta)$ & $\theta_1$ & $\theta_2$ & \ldots & $\theta_n$ \\
\hline $\omega_1$ & $\mathsf P_{11}$ & $\mathsf P_{12}$ & \ldots & $\mathsf P_{1n}$ \\
\hline $\omega_2$ & $\mathsf P_{21}$ & $\mathsf P_{22}$ & \ldots & $\mathsf P_{2n}$ \\
\hline \ldots & \ldots & \ldots & \ldots & \ldots \\
\hline $\omega_m$ & $\mathsf P_{m1}$ & $\mathsf P_{m2}$ & \ldots & $\mathsf P_{mn}$
\end{tabular}
\end{center}
А тогда, для эксперимента $\Omega$ при условии, что эксперимент $\Theta$ оказался в состоянии $\theta_j$, где $j \in \{1, 2, \ldots, n\}$ могут быть составлены следующие $n$ таблиц условных вероятностей:
\begin{center}
\begin{tabular}{c|c|c|c|c}
	$(\Omega, \theta_j)$ & $(\omega_1|\theta_j)$ & $(\omega_2|\theta_j)$ & $\ldots$ & $(\omega_m|\theta_j)$\\
	\hline $\mathsf P$ & $\mathsf{P}(\omega_1|\theta_j)$ & $\mathsf{P}(\omega_2|\theta_j)$ & $\ldots$ & $\mathsf{P}(\omega_m|\theta_j)$
\end{tabular}.
\end{center}
Понятно, что каждая из таблиц описывает эксперимент, для которого можно вычислить неопределенность Джини. Как и в случае с энтропией, введем следующее определение.
\begin{definition}
Условной неопределенностью Джини эксперимента $\Omega$ при условии, что эксперимент $\Theta$ оказался в состоянии $\theta_j$, $j \in \{1, 2, ..., n\}$ называется число
$$
\mathsf{G}(\Omega|\theta_j) = 1 - \sum \limits_{i=1}^m  \mathsf{P}^2(\omega_i|\theta_j).
$$
\end{definition}
В результате, для каждого состояния $\theta_j$, которое происходит с некоторой вероятностью $\mathsf{P}(\theta_j)$, неопределенность Джини принимает значения $\mathsf{G}(\Omega|\theta_j)$, а значит является случайной величиной с рядом распределения:
\begin{center}
\begin{tabular}{c|c|c|c}
$\mathsf G(\Omega| \theta_j)$ & $\mathsf G(\Omega| \theta_1)$ & \ldots & $\mathsf G(\Omega| \theta_n)$ \\
\hline $\mathsf P$ & $\mathsf P(\theta_1)$ & \ldots & $\mathsf P(\theta_n)$
\end{tabular}.
\end{center}
Так, полученную систему хорошо характеризует так называемая взвешенная неопределенность Джини.
\begin{definition}
Взвешенной неопределенностью Джини эксперимента $\Omega$ при условии, что произошел эксперимент $\Theta$, называется величина	
$$
\mathsf G(\Omega | \Theta) = \mathsf E \left(\mathsf G(\Omega| \theta_j) \right) = \sum\limits_{j = 1}^n \mathsf P(\theta_j) \mathsf G(\Omega| \theta_j).
$$	
\end{definition}
Прирост информации, основывающийся на понятии неопределенности Джини, вводится следующим образом.
\begin{definition}
Приростом Джини (Gini Gain) называется величина
$$
\mathsf{GG}(\Omega | \Theta) = \mathsf{G}(\Omega) - \mathsf{G}(\Omega | \Theta).
$$
\end{definition}
Значение прироста Джини используют при построении деревьев принятия решений аналогично тому, как мы использовали прирост информации: каждый раз на первом шаге алгоритма выбирается наибольший прирост Джини.

\subsection{Небольшое сравнение прироста Джини и энтропии}
На практике рассмотренные критерии прироста информативности дают почти одинаковый результат. В этом легко убедиться, построив, для примера, графики функций энтропии и удвоенной неопределенности Джини в случае двух исходов:
$$
I_{Entropy} = - p \log_2 p - (1 - p)\log_2 (1 - p),
$$
$$
I_{Gini} = 2 \cdot \left( p(1 - p) + (1-p)(1 - (1-p)) \right).
$$
Удвоение неопределенности Джини позволяет нормировать значения функции $I_{Gini}$ до единицы, что, условно, <<уравнивает единицы измерения>>. Как видно рисунка \ref{E_vs_G}, неопределенность системы с двумя исходами для различных значений вероятности сопоставимы. 

Таким образом, нет явных предпосылок использовать конкретный метод измерения информативности, так как они оба справляются со своей задачей и дают почти одинаковый результат. Однако вычисление энтропии -- задача более затратная (с точки зрения вычислений) из-за присутствующего в определении логарифма. Именно по этой причине часто, по умолчанию, в инструментах используется неопределенность Джини. 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{H_vs_G}
\caption{Сравнение энтропии и неопределенности Джини.}
\label{E_vs_G}
\end{figure}


\subsection{Прирост Джини на данных}
Вернемся к примеру с кошками. Перед вами таблица, содержащая информацию о породе, цвете шерсти и росте в холке, которые влияют на привлекательность конкретной кошки, участвующей в соревнованиях.
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Порода} & \textbf{Шерсть} & \textbf{Рост} & \textbf{Привлекательность} \\ \hline
Британец & Белый & Высокий & Нет \\ \hline
Британец & Серый & Высокий & Да \\ \hline
Британец & Белый & Низкий & Да \\ \hline
Мейн-кун & Белый & Высокий & Нет \\ \hline
Мейн-кун & Коричневый & Высокий & Да \\ \hline
Мейн-кун & Коричневый & Низкий & Нет \\ \hline
Рэгдолл & Серый & Высокий & Да \\ \hline
Мейн-кун & Серый & Низкий & Да \\ \hline
\end{tabular}
\end{center}
Условные вероятности отнесения к целевому классу <<Привлекательность>> в зависимости от породы были найдены ранее, они равны:
$$
\mathsf{P}(\text{Привлекательность = Да} | \text{Порода = Британец}) = \frac{2}{3}, 
$$
$$
\mathsf{P}(\text{Привлекательность = Нет} | \text{Порода = Британец}) = \frac{1}{3},
$$
$$
\mathsf{P}(\text{Привлекательность = Да} | \text{Порода = Мейн-кун}) = \frac{1}{2},
$$
$$
\mathsf{P}(\text{Привлекательность = Нет} | \text{Порода = Мейн-кун}) = \frac{1}{2},
$$
$$
\mathsf{P}(\text{Привлекательность = Да} | \text{Порода = Рэгдолл}) = 1
$$
$$
\mathsf{P}(\text{Привлекательность = Нет} | \text{Порода = Рэгдолл}) = 0.
$$
Тогда неопределенность Джини для каждой породы будет принимать следующие значения:
$$
\mathsf{G}(\text{Привлекательность}|\text{Порода = Британец})=1-\left(\left(\frac{2}{3}\right)^{2}+\left(\frac{1}{3}\right)^{2}\right)=\frac{4}{9},
$$
$$
\mathsf{G}(\text{Привлекательность}|\text{Порода = Мейн-кун})=1-\left(\left(\frac{1}{2}\right)^{2}+\left(\frac{1}{2}\right)^{2}\right)=\frac{1}{2},
$$
$$
\mathsf{G}(\text{Привлекательность}|\text{Порода = Рэгдолл})=1-\left(1^{2}+0^{2}\right)=0.
$$
При этом состояния, то есть выбор конкретной породы, определяются с некоторой вероятностью, а именно: $\frac{3}{8}$, $\frac{4}{8}$ и $\frac{1}{8}$ соответственно. Запишем полученные значения в таблицу:
\begin{center}
\begin{tabular}{c|c|c|c}
$\mathsf G(\text{Привлекательность}|\text{Порода})$ & $\frac{4}{9}$ & $\frac{1}{2}$ & $0$ \\
\hline $\mathsf P$ & $\frac{3}{8}$ & $\frac{4}{8}$ & $\frac{1}{8}$
\end{tabular}.
\end{center}
Окончательно, взвешенная неопределённость Джини составит:
$$
\mathsf{G}(\text{Привлекательность}|\text{Порода}) =\frac{3}{8} \cdot \frac{4}{9}+\frac{4}{8} \cdot \frac{1}{2}+\frac{1}{8} \cdot 0=\frac{5}{12}.
$$
При этом неопределенность Джини, эксперимента <<Привлекательность>> равна:
$$
\mathsf{G}(\text{Привлекательность}) = 1 - \left ( \left (\frac{5}{8}\right )^2 + \left (\frac{3}{8}\right )^2 \right ) = \frac{15}{32},
$$
а значит прирост Джини:
$$
\mathsf{GG}(\text{Привлекательность}|\text{Порода}) = \frac{15}{32} - \frac{5}{12} = \frac{5}{96} \approx 0.052.
$$
Аналогичным образом рассчитываются прирост Джини для всех атрибутов. 
Конечно, атрибут -- шерсть, в такой эвристике тоже лучше всего характеризует прирост информации, как и в случае с энтропией.


