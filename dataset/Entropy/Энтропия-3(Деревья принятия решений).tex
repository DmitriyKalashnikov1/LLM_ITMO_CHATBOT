\documentclass[a4paper,14pt]{extarticle}


\usepackage[left = 20mm, right = 20mm, top = 25mm, bottom = 25mm, headheight=30pt]{geometry}
\usepackage{amssymb}
\usepackage{cmap}
\usepackage{enumerate}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{euscript}
\usepackage{graphics}
\usepackage[T1, T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage{pgf,tikz}
\usepackage{multicol}
\usepackage{float}
\restylefloat{table}
\usepackage{multirow}
\usepackage{caption}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{fancyhdr} 
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[LE,RO]{Университет ИТМО}
\fancyhead[RE,LO]{Высшая школа цифровой культуры}
\fancyfoot[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{2pt}


\sloppy



\usepackage{xcolor} % Required for specifying custom colours
\definecolor{grey}{rgb}{0.9,0.9,0.9} % Colour of the box surrounding the title
%\usepackage[sfdefault]{ClearSans} % Use the Clear Sans font (sans serif)
%\usepackage{XCharter} % Use the XCharter font (serif)
\definecolor{foo}{HTML}{EFF5F9}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\Argmin}{Arg\,min}
\DeclareMathOperator*{\Argmax}{Arg\,max}
\DeclareMathOperator*{\logloss}{logloss}
\newcommand{\eqdef}{\overset{\mathrm{def}}{=\joinrel=}}
\def\re{{\rm Re}}
\def\im{{\rm Im}}
\def\dim{\rm dim}
\def\Ext{\rm Ext}
\def\wt#1{{{\widetilde #1} }}
\def\wh#1{{{\,\widehat #1\,} }}
\def\graph{{\rm gr\,}}
\def\ran{{\rm ran\,}}
\def\dom{{\rm dom\,}}
\def\ker{{\rm ker\,}}
\def\supp{{\rm supp\,}}
\def\diag{{\rm diag\,}}

\newcommand\dN{{\mathbb{N}}}
\newcommand\dR{{\mathbb{R}}}
\newcommand\dC{{\mathbb{C}}}
\newcommand{\bO}{{\mathbb{O}}}
\newcommand{\bU}{{\mathbb{U}}}
\newcommand\dZ{{\mathbb{Z}}}

\newcommand\gotB{{\mathfrak{B}}}
\newcommand\gotD{{\mathfrak{D}}}
\newcommand\gotH{{\mathfrak{H}}}
\newcommand\gotK{{\mathfrak{K}}}
\newcommand\gotL{{\mathfrak{L}}}
\newcommand\gotM{{\mathfrak{M}}}
\newcommand\gotN{{\mathfrak{N}}}
\newcommand\gotR{{\mathfrak{R}}}
\newcommand\gotS{{\mathfrak{S}}}
\newcommand\gotT{{\mathfrak{T}}}
\newcommand\gott{{\mathfrak{t}}}
\newcommand\gotC{{\mathfrak{C}}}
\newcommand\gotZ{{\mathfrak{Z}}}
\newcommand{\RNumb}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\newcommand{\ga}{{\alpha}}
\newcommand{\gd}{{\delta}}
\newcommand{\gD}{{\Delta}}
\newcommand{\gga}{{\gamma}}
\newcommand{\gG}{{\Gamma}}
\newcommand{\gF}{{\Phi}}
\newcommand{\gf}{{\phi}}
\newcommand{\gk}{{\kappa}}
\newcommand{\gK}{{\Kappa}}
\newcommand{\gl}{{\lambda}}
\newcommand{\gL}{{\Lambda}}
\newcommand{\gO}{{\Omega}}
\newcommand{\go}{{\omega}}
\newcommand{\gs}{{\sigma}}
\newcommand\gS{{\Sigma}}
\newcommand{\gT}{{\Theta}}
\newcommand{\gY}{{\Upsilon}}


\newcommand\cA{{\mathcal{A}}}
\newcommand\cB{{\mathcal{B}}}
\newcommand\cC{{\mathcal{C}}}
\newcommand\cD{{\mathcal{D}}}
\newcommand\cH{{\mathcal{H}}}
\newcommand\cK{{\mathcal{K}}}
\newcommand\cM{{\mathcal{M}}}
\newcommand\cN{{\mathcal{N}}}
\newcommand\cO{{\mathcal{O}}}
\newcommand\cP{{\mathcal{P}}}
\newcommand\cT{{\mathcal{T}}}
\newcommand\cU{{\mathcal{U}}}
\newcommand\cZ{{\mathcal{Z}}}

\newcommand\Rg{{\rm Rg}}
\newcommand\res{{\rm res}}
\newcommand\pr{{\rm Pr}}

\DeclareMathOperator{\sign}{sign}

\newtheorem{theorem}{Теорема}[subsection]
\newtheorem{lemma}{Лемма}[subsection]
\newtheorem{corollary}[theorem]{Следствие}
\newtheorem{definition}{Определение}[subsection]
\newtheorem{example}{Пример}[subsection]
\newtheorem{remark}{Замечание}[subsection]

\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}

\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}

\newcommand{\bead}{\begin{eqnarray*}}
\newcommand{\eead}{\end{eqnarray*}}

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}

\newcommand{\bed}{\begin{displaymath}}
\newcommand{\eed}{\end{displaymath}}

\newcommand{\bl}{\begin{lemma}}
\newcommand{\el}{\end{lemma}}

\newcommand{\bt}{\begin{theorem}}
\newcommand{\et}{\end{theorem}}

\newcommand{\bc}{\begin{corollary}}
\newcommand{\ec}{\end{corollary}}

\newcommand{\br}{\begin{remark}}
\newcommand{\er}{\end{remark}}

\newcommand{\bd}{\begin{definition}}
\newcommand{\ed}{\end{definition}}

\newcommand{\bspi}{\begin{split}}
\newcommand{\espi}{\end{split}}

\newcommand{\la}{\label}
\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}


\newenvironment{proof}%
{\begin{sloppypar}\noindent{\bf Доказательство.}}%
{\hspace*{\fill}$\square$\end{sloppypar}}
\renewcommand{\Large}{\fontsize{16}{25pt}\selectfont}

\newcommand{\slim}{\,\mbox{\rm s-}\hspace{-2pt} \lim}
\newcommand{\wlim}{\,\mbox{\rm w-}\hspace{-2pt} \lim}
\newcommand{\olim}{\,\mbox{\rm o-}\hspace{-2pt} \lim}
\newcommand{\transpose}[1]{\ensuremath{#1^{\scriptscriptstyle t}}}




\graphicspath{ {img/} }

\usepackage{indentfirst} 
\setlength\parindent{1cm}

\usepackage{textcase} 
\usepackage{titlesec}

\usepackage{indentfirst} 
\setlength\parindent{1cm}
% НАСТРОЙКИ ЗАГОЛОВКОВ
\usepackage{textcase} 
\usepackage{titlesec}
\titleformat{\section}[block]{\sffamily\Large\bfseries\filcenter}{\thesection}{0.5em}{}
\titleformat{\subsection}[block]{\large\sffamily\bfseries}{\thesubsection}{0.5em}{}
\titlespacing{\subsection}{2cm}{1mm}{3mm}

\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	
	%------------------------------------------------
	%	Grey title box
	%------------------------------------------------
	\begin{center}
	\includegraphics[width=0.4\textwidth]{dc.png}
    \end{center}
    
    	\vfill
    	
	\colorbox{foo}{
		\parbox[t]{0.93\textwidth}{ % Outer full width box
			\parbox[t]{0.91\textwidth}{ % Inner box for inner right text margin
				\raggedleft % Right align the text
				%\fontsize{50pt}{80pt}\selectfont % Title font size, the first argument is the font size and the second is the line spacing, adjust depending on title length
				\vspace{0.7cm} % Space between the start of the title and the top of the grey box
				
				\huge Энтропия\\ Деревья принятия решений
				
				\vspace{0.7cm} % Space between the end of the title and the bottom of the grey box
			}
		}
	}
	
	\vfill % Space between the title box and author information
	
	%------------------------------------------------
	%	Author name and information
	%------------------------------------------------

    

	
	\parbox[t]{0.93\textwidth}{ % Box to inset this section slightly
		\raggedleft % Right align the text
		%\large % Increase the font size
		{Высшая Школа Цифровой Культуры}\\[4pt] % Extra space after name
		Университет ИТМО\\[4pt] % Extra space before URL
		dc@itmo.ru\\[4pt]
		
		\hfill\rule{0.2\linewidth}{1pt}% Horizontal line, first argument width, second thickness
	}
	
\end{titlepage}

%----------------------------------------------------------------------------------------


\newpage
%
\pagestyle{empty}
\tableofcontents
%
\clearpage
\pagestyle{fancy}


\section{Деревья принятия решений}
Итак, мы рассмотрели способ измерения прироста информации с использованием понятия энтропии. Теперь мы готовы перейти к построению дерева принятия решений, однако для начала неплохо бы определиться: а что это такое?

\subsection{Немного о самих деревьях}
Если вы знакомы с теорией графов, то определение дерева принятия решений может быть дано следующим образом. 
\begin{definition}
Дерево принятия решений -- это дерево, то есть ациклический связный граф, имеющее следующие метки:
\begin{itemize}
\item узлы дерева, не являющиеся листьями -- это атрибуты.
\item в листьях дерева находятся отклики -- результаты классификации.
\item на ребрах находится правило -- значение узла (атрибута), из которого исходит ребро.
\end{itemize}
\end{definition}
Даже если вы не знакомы с теорией графов, то введенное понятие прекрасно иллюстрируются на уже знакомом нам дереве принятия решений, цель которого -- классифицировать предложения о работе на два типа: те, которые стоит принять, и те, которые стоит отклонить. Как видно из рисунка, из верхнего узла дерева, отвечающего за предиктор (или атрибут) <<размер заработной платы>>, выходит три ребра. Если значение узла меньше, чем $50000$, то мы сразу попадаем в лист: предложение предлагается отклонить. Если значение узла больше или равно, чем
$100000$, то мы снова попадаем в лист, но уже с другим откликом: предложение предлагается принять. Если же значение узла находится в диапазоне от $50000$ включительно до $100000$ не включительно, то ребро нас ведет в следующий узел, который отвечает за время, требующееся на дорогу до работы. В зависимости от значения этого узла, возникают дополнительные ветвления, или ветки дерева, которые вы легко отследите самостоятельно. Обратите внимание, любой путь по построенному дереву заканчивается листом, и в каждый лист можно попасть из начальной (корневой) вершины дерева -- это и есть требование связности в определении. Ацикличность же, или отсутствие циклов, обеспечивается тем, что как в каждый узел (кроме изначального), так и в каждый лист входит лишь одно ребро.
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{tree_example.png}
\caption{Пример дерева решений.}
\label{tree_example}
\end{figure}

\begin{definition}
Глубиной конкретного листа (или узла) в дереве называется количество ребер, которые соединяют его с корневой вершиной. Глубиной дерева называют максимум из глубин его листов.
\end{definition}
В нашем примере всего $5$ листов. Глубина каждого из них равна (слева-направо): $1$, $3$, $3$, $2$ и $1$, соответственно, а значит глубина дерева равна $3$.

Разобравшись с тем, а что такое дерево принятия решений, а также как измерять информативность признаков, перейдем к рассмотрению конкретных алгоритмов построения деревьев принятия решений.

\subsection{Алгоритм построения дерева и пример с кошками}
Составим алгоритм построения дерева принятия решений на тренировочном наборе данных $x_1, x_2, ..., x_n$, состоящем из $n$ элементов c $p$ предикторами $X_1, X_2, ..., X_p$ каждый, и откликом $Y$.
\begin{enumerate}
	\item Пусть на вход подается множество объектов $X$. Среди $p$ предикторов выбрать тот, для которого прирост информации максимален. Итак, решается задача
	$$
	\argmax\limits_{Q \in \{X_1, X_2, ..., X_p\}} \mathsf{IG}(Y|Q)
	$$	
	\item Пусть выбран предиктор $X_i$, принимающий на наборе данных $X$ ровно $t$ уникальных значений. Выполнить разделение набора данных $X$ на подмножества $S_1, \ldots, S_t$ по уникальным значениям предиктора $X_i$. 
	\item Для каждого множества $S_i$, $i \in \{1, 2, ..., t \}$, если энтропия по отклику не равна нулю, повторить шаги $1$ и $2$.
\end{enumerate}
Если использовать введенную выше терминологию деревьев принятия решений, то выбранный на первом шаге алгоритма предиктор $X_i$ -- это узел дерева, а разделение множества объектов на $t$ подмножеств по уникальному значению предиктора -- это установление $t$ ребер из узла $X_i$. Равенство же нулю энтропии для некоторого множества $S_i$ означает, что установленное ребро, в результате которого получилось множество $S_i$, указывает на лист, которому нужно присвоить значение откликов объектов из $S_i$ (ведь раз энтропия ноль, то все объекты имеют одинаковый отклик).

Опробуем алгоритм на уже озвученном примере с кошками. Итак, таблица исходных данных в наших обозначениях такова:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
№ & \textbf{Порода} ($X_1$) & \textbf{Шерсть} ($X_2$) & \textbf{Рост} ($X_3$) & \textbf{Привлекательность} ($Y$) \\ \hline
$x_1$ & Британец & Белый & Высокий & Нет \\ \hline
$x_2$ & Британец & Серый & Высокий & Да \\ \hline
$x_3$ & Британец & Белый & Низкий & Да \\ \hline
$x_4$ & Мейн-кун & Белый & Высокий & Нет \\ \hline
$x_5$ & Мейн-кун & Коричневый & Высокий & Да \\ \hline
$x_6$ & Мейн-кун & Коричневый & Низкий & Нет \\ \hline
$x_7$ & Рэгдолл & Серый & Высокий & Да \\ \hline
$x_8$ & Мейн-кун & Серый & Низкий & Да \\ \hline
\end{tabular}.
\end{center}
Иными словами, каждая строчка таблицы -- это тренировочный объект, обладающий тремя предикторами $X_1, X_2, X_3$ -- <<Порода>>, <<Шерсть>> и <<Рост>>, соответственно, и откликом $Y$ -- <<Привлекательность>>.

Первый шаг алгоритма нами уже был проделан ранее. Согласно разобранному примеру, самым информативным оказался критерий <<Шерсть>>, или предиктор $X_2$. Так как $X_2$ принимает три уникальных значения: <<Белый>>, <<Серый>> и <<Коричневый>>, то, согласно второму шагу алгоритма, весь набор тренировочных данных нужно разделить на $3$ группы (по значению $X_2$). Пусть первая группа $S_{\text{Коричневый}}$ -- группа кошек с коричневой шерстью, тогда в нее входят $x_5, x_6$. Во вторую группу $S_{\text{Белый}}$, с белой шерстью, входят $x_1, x_3, x_4$. В последнюю же группу $S_{\text{Серый}}$, с серой шерстью, входят все остальные, то есть $x_2, x_7, x_8$. Давайте это визуализируем.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{cats_tree.png}
\caption{Дерево принятия решений для задачи о кошках.}
\label{tree_1}
\end{figure}

Посмотрите на рисунок \ref{tree_1} (он несколько отличается по визуализации и информативности от ранее описанных деревьев). В верхнем белом блоке (корневом узле дерева) содержится информация об исходном наборе данных $X = \{x_1, x_2, ..., x_n\}$. Во второй строке этого блока указана энтропия -- это энтропия, вычисленная по отклику $Y$ в исходном наборе данных. Она, как нами было показано, примерно равна $0.954$. В третьей строке указано количество элементов исследуемого набора данных, а в четвертой -- соотношение между привлекательными и не привлекательными кошками в исходном наборе данных (непривлекательных $3$, привлекательных $5$).

Так как <<Шерсть>> -- самый информативный признак на первом этапе, принимающий $3$ различных значения, то из корневого узла выходит три ребра, и появляется второй уровень дерева, состоящий из трех групп: $S_{\text{Коричневый}}$, $S_{\text{Белый}}$, $S_{\text{Серый}}$, отвечающих трем различным узлам. Так как среди кошек с коричневой шерстью $1$ привлекательная и $1$ нет, то энтропия по отклику <<Привлекательность>> в наборе данных $S_{\text{Коричневый}}$ равна $1$. Аналогично, так как среди кошек с белой шерстью $2$ непривлекательных и $1$ привлекательная, то энтропия по отклику <<Привлекательность>> в наборе данных $S_{\text{Белый}}$ равна
$$
-\frac{2}{3}\log_2 \frac{2}{3} - \frac{1}{3}\log_2 \frac{1}{3} \approx 0.918.
$$ 
Среди кошек с серой шерстью все три оказались привлекательными, а потому энтропия по отклику <<Привлекательность>> в группе $S_{\text{Серый}}$ равна $0$, и дальнейшее разбиение на подмножества группы $S_{\text{Серый}}$, в отличие от остальных двух групп, не требуется, а узел, соответствующий этому блоку, становится листом с откликом <<Да>>.

В других же подмножествах остается неопределенность, а значит, в соответствии с алгоритмом, нужно определить новые критерии разбиения для каждого из них. Если внимательно посмотреть на подмножества $S_{\text{Коричневый}}$ и $S_{\text{Белый}}$ (ну или если просто посчитать), то становится понятно, что наиболее информативным критерием в каждой группе будет рост в холке. Отсюда и разделение на третьем уровне дерева. Легко видеть, что теперь каждое образовавшееся подмножество (их всего 4) имеет нулевую энтропию,  значит алгоритм завершен, а все узлы последнего уровня становятся листьями с откликами (слева-направо) <<Да>>, <<Нет>>, <<Нет>> и <<Да>>, соответственно.

Классификация новых объектов, имея построенное дерево, выполняется крайне просто. Новый объект сначала проходит классификацию по верхнему уровню (в нашем примере -- по цвету шерсти). Если цвет шерсти серый, то дерево решений говорит, что кошка точно понравится судьям. Если же цвет коричневый или белый, то далее нужно смотреть на рост кошки (на следующий уровень). Согласно дереву, коричневая высокая или белая низкая кошка понравится судьям, а коричневая низкая или белая высокая -- не понравится.

\begin{remark}\label{restrictions}
Последний пункт описанного в начале алгоритма -- это лишь одно из возможных условий остановки процесса деления набора тренировочных данных на группы (и, как следствие, одно из возможных условий остановки построения ДПР). Ситуация, когда энтропия каждой получившейся группы равна нулю означает, что в каждой группе все элементы имеют один и тот же отклик, а значит понятно и как эту группу <<охарактеризовать>> -- по отклику. В то же время, особенно на больших объемах данных, разделение до нулевой энтропии -- дело достаточно трудоемкое, зачастую приводящее к переобучению. Поэтому часто используют следующие критерии остановки:
\begin{itemize}
	\item ограничение глубины дерева. Узлы, имеющие максимальную установленную глубину, становятся листами;
	\item ограничение минимального количества элементов в группе. Если при разделении рассматриваемого множества на подмножества по уникальным значениям предиктора, получающиеся группы тренировочных данных содержат меньше элементов, чем задано исследователем, то деление останавливается. В терминах ДПР это означает, что текущий узел становится листом;
	\item достижение в группе какого-то заданного критерия: например, значения неопределенности. В терминах ДПР это опять же означает, что текущий узел становится листом.
\end{itemize}
В каждом из описанных случаев, однако, остается открытым вопрос: какое же значение приписать листу? Обычно ему приписывают тот отклик, чьих представителей в соответствующей группе тренировочных данных большинство. Подробнее об этом мы поговорим чуть позже.
\end{remark}

 
\subsection{Бинарное дерево решений}
Рассмотренный алгоритм построения дерева принятия решения удобен в том случае, когда мы работаем с предикторами, все возможные значения которых присутствуют в тренировочных данных. Если тестовый объект обладает значением предиктора, которого не было в тренировочных данных, то описанный алгоритм классификации, очевидно, не работает. Скажем, пусть нас интересует следующий вопрос: понравится ли судьям черная невысокая кошка? Взглянув на построенное ранее дерево (рисунок \ref{tree_1}), мы понимаем, что уже на первом уровне наша классификация ломается: по какой ветке идти? Значение признака <<Шерсть>> не подходит ни под один из приведенных вариантов.

Озвученная проблема является, на самом деле, весьма серьезной. Дело в том, что нередко мы не знаем все возможные значения, которые может принимать атрибут рассматриваемых объектов, а часто бывает и так, что этих значений и вообще -- бесконечное число (особенно, если значения атрибутов -- числовые). Примерами числовых атрибутов, принимающих бесконечное число значений, могут служить: рост человека, текущее время, дата, масса товара на весах на рынке и многое-многое другое. Если же атрибут не числовой, он редко может принимать бесконечное число значений, но может принимать <<очень много>> значений. Примером могут служить названия городов по всему миру. 

Кроме того, чем больше значений атрибутов мы имеем, тем дольше строится и само решающее дерево. К тому же, оно становится очень <<ветвистым>>, что мешает как интерпретации модели в целом, так и способствует ее, модели, переобучению.
Обозначенных причин, наверное, достаточно, чтобы понять, что требуется разработать какой-то новый подход к построению решающих деревьев, обладающий следующими преимуществами:
\begin{itemize}
\item возможность классификации объектов с ранее (то есть при обучении) не встречавшимися значениями предикторов;
\item возможность объединять (группировать, укрупнять) множество значений признаков.
\end{itemize}
Выполнение последнего требования, как мы уже сказали, способствует уменьшению количества ветвлений и, как следствие, более простой интерпретируемости и более высокой скорости работы модели.

Все эти размышления приводят нас к так называемым бинарным деревьям принятия решений, или к бинарным решающим деревьям. 

\begin{definition}
	Бинарное дерево принятия решений -- это дерево принятия решений, из каждого узла которого выходит ровно два ребра.
\end{definition}
Итак, бинарные деревья принятия решений -- это все те же деревья принятия решений, главная особенность которых состоит в том, что разделение осуществляется всегда на два подмножества, в каждом из которых заново рассчитывается энтропия и по проделанным расчетам определяется наиболее информативный критерий дальнейшего разбиения (если это разбиение требуется). Разделение же осуществляется по принципу: если значения атрибута удовлетворяют <<установленному>> условию, то объект относится к первому множеству, а если не удовлетворяют -- ко второму. Таким образом, объекты, значения признаков которых ранее не встречались, все равно будут отнесены к одному из двух возможных подмножеств, а значит в итоге будут классифицированы. Но как проводить разделение? Что это за <<установленные условия>>?

\subsection{Типы признаков и их группировка}
Начнем с некоторых важных определений. Еще раз отметим, что мы работаем с тренировочным набором данных $x_1, x_2, ..., x_n$ объема $n$, каждый объект которого обладает $p$ атрибутами
$$
x_i = (x_{i1}, x_{i2}, ..., x_{ip}).
$$

\begin{definition}
Признак объекта называется категориальным, если он принимает конечное число значений.
\end{definition}
Категориальные признаки могут быть как числовыми, так и не числовыми. В рассмотренном примере с кошками все признаки -- категориальные. Признак <<Шерсть>> принимает какое-то значение из конечного множества $\{\text{Белый}, \text{Серый}, \text{Коричневый}\}$, а <<Рост>> -- из конечного множества $\{\text{Высокий}, \text{Низкий}\}$. Кроме категориальных, оказывается удобным выделять и некатегориальные признаки.
\begin{definition}
Если признак объекта не является категориальным, то мы будем называть его некатегориальным.	
\end{definition}
Примерами некатегориальных признаков, как мы уже упоминали ранее, являются: рост, текущее время, масса товара на весах в магазине и многие другие. Некатегориальные признаки чаще всего являются числовыми. 

Теперь давайте решим, как можно объединять признаки в группы. Для некатегориального числового признака $X_i$ часто поступают вот каким образом: множество его значений делят на два множества:
$$
\text{первое множество: } X_i \leq C, \ \text{второе множество: } X_i > C,
$$
причем число $C$ (или так называемое пороговое значение) подбирается отдельно. 
\begin{remark}
Давайте чуть подробнее поговорим про выбор числа $C$. Как этот выбор осуществляется? Тут возможно много вариантов, мы же приведем лишь несколько.  
\begin{enumerate}
\item Первый вариант такой. Для начала определяют множество значений (обычно отрезок) $[a, b]$ некатегориального числового признака $X_i$ либо исходя из каких-то теоретических соображений, либо исходя из значений этого признака у тренировочных данных. В последнем случае в качестве $a$ часто берут наименьшее значение рассматриваемого признака среди тренировочных данных, а в качестве $b$ -- наибольшее. Следующий этап -- это задание шага $h > 0$ изменения значения $C$. При выбранном шаге $h$, в качестве значений $C$ разумно рассмотреть следующие: 
$$
\{a, a + h, ..., a + (k-1)h, a + k h\}, \ k \in \mathbb{Z},
$$
где $a + (k-1)h < b$, но  $a + kh \geq b$. Для каждого из выбранных значений вычисляют прирост информации, отвечающий полученному разделению на два класса: $X_i \leq C$ и $X_i > C$. 
\item Второй вариант похож на первый. Значения $C$ выбираются таким образом, чтобы при каждом следующем значении $C$ к множеству $X_i \leq C$ добавлялось ровно одно новое значение предиктора $X_i$. Формально это можно описать следующим образом. Пусть $\{x_{1i}, x_{2i}, ..., x_{ti}\}$, $1 \leq t \leq n$ -- множество всех возможных различных значений предиктора $X_i$ на тренировочных данных объема $n$. Предположим также, что они упорядочены по возрастанию, то есть
$$
x_{1i} < x_{2i} < ... < x_{ti}.
$$
Тогда в качестве значений $C$ разумно взять следующие:
$$
\{C_1, C_2, ..., C_t\},
$$
где $C_t \geq x_{ti}$, $x_{ji} \leq C_j < x_{(j+1)i}$, $j \in \{1, 2, ..., (t-1)\}$.
\end{enumerate}
В итоге выбирают то разделение, при котором прирост информации наибольший.
\end{remark}

Если же признак $X_i$ является категориальным и принимает значения из множества $M = \{x_{1i}, x_{2i}, ..., x_{ti}\}$, $t \in \{1, 2, ..., n\}$, то его множество значений можно разделить на две части, например, так:
$$
\text{первое множество: } X_i \in M_1, \ \text{второе множество: } X_i \in M_2,
$$
где $M_1 \cap M_2 = \varnothing$, $M_1 \cup M_2 = M$, $M_j \neq \varnothing$, $j \in \{1, 2\}$. Иными словами, все множество $M$ уникальных значений признака $X_i$ разбивается на $2$ непустые непересекающиеся части.
\begin{remark}
	В библиотеках, реализованных в различных инструментах, чаще всего множество $M_1$ составляется ровно из одного значения признака $X_i$, тогда как $M_2$ составляется из всех остальных значений. Тем самым, если $M = \{x_{1i}, x_{2i}, ..., x_{ti}\}$, $t \in \{1, 2, ..., n\}$, то 
	$$
	M_1 = \{C\}, \ M_2 = M \setminus M_1,
	$$
где на каждой итерации $C$ принимает по очереди значения каждого элемента множества $M$. В итоге выбирают то разделение, при котором прирост информации наибольший.
\end{remark}


\subsubsection{Алгоритм построения бинарного дерева решений и пример с кошками}

Составим алгоритм построения бинарного дерева для тренировочного набора данных $X = \{x_1, x_2, ..., x_n \}$ объема $n$ c $p$ предикторами $X_1, X_2, \ldots, X_p$ и откликом $Y$. 
\begin{enumerate}
	\item Пусть на вход подается множество объектов $X$. Среди $p$ предикторов выбрать тот, для фиксированного разбиения значений которого на подмножества достигается наибольший прирост информации. Итак, решается задача 
	$$
	\argmax\limits_{Q \in \{X_1, X_2, ..., X_p\}} \mathsf{IG}(Y|Q(C)),
	$$	
	где $C$ определяется в зависимости от типа признака, согласно описанным выше правилам.
	\item Пусть выбран предиктор $X_i$ и найдено $C$. Выполнить разделение по выбранному признаку на два подмножества $S_{C}$ и $\overline{S}_{C}$. В первое подмножество входят объекты, значения $i$-ого атрибута которых удовлетворяют критерию разделения, во второе -- все остальные.
	\item Повторить шаги 1 и 2 для множеств $S_C$ и $\overline{S}_C$ либо согласно с ограничением числа уровней (согласно замечанию \ref{restrictions}), либо до нулевой энтропии в каждой из групп, если ограничение не задано.
\end{enumerate}
Опробуем алгоритм все на тех же кошках. Так, согласно выполненным ранее вычислениям, предиктор <<Шерсть>> со значением <<Серый>> обеспечивал нулевую энтропию (то есть максимальный прирост информации). Этот критерий и будет первым при формировании двух подмножеств $S_{\text{Серый}}$ и $\overline{S}_{\text{Серый}}$. В первое множество войдут все кошки с серой шерстью, во второе -- все остальные. Энтропия во втором множестве составит
$$
-\frac{3}{5}\log_2 \frac{3}{5} - \frac{2}{5}\log_2 \frac{2}{5} \approx 0.971.
$$
В первом подмножестве неопределенности нет, а вот во втором неопределенность сохраняется. Согласно третьему шагу алгоритма это означает, для этого подмножества необходимо повторить первые два шага алгоритма.
Дальнейшее разбиение подмножества и критерии можно увидеть на рисунке \ref{tree_bin}. Оказывается, на втором уровне самым информативным снова является разделение кошек по цвету, но теперь уже на <<Белых>> и <<Не белых>>. После этого, на последнем уровне, кошки делятся в зависимости от размера. Легко видеть, что на последнем уровне дерева все подмножества имеют нулевую энтропию, что означает, что построение завершено.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{cats_bin_tree.png}
\caption{Пример бинарного дерева решений.}
\label{tree_bin}
\end{figure}

Бинарное дерево, как мы уже отмечали, позволяет проводить классификацию новых объектов даже в случае, когда значение какого-то предиктора оказалось уникальным (или новым). Так, классификация невысокой черной кошки в случае дерева принятия решений, построенного ранее, была невозможна. В то же время, если рассмотреть только что построенное бинарное дерево, классификация возможна. Так как черная кошка -- не серая, то идем по левой ветке. Опять, так как черная кошка -- не белая, снова идем по левой ветке. Ну и, наконец, так как рассматриваемая кошка -- не высокая, то идем по правой ветке и делаем вывод, что кошка не понравится судьям.

\subsubsection{Синтетический пример}
Опробуем алгоритм на синтетических данных с двумя предикторами $X_1$ и $X_2$, причем на таких, которые интуитивно разбиваются прямой на два подмножества. Предикторы, отвечающие рассматриваемым данным, принимают лишь числовые значения. 

Задача все та же -- провести двухклассовую классификацию. У данных с откликом $0$ (зеленые) предикторы независимы и имеют распределение $\mathsf{N}_{0, 1}$. Таких точек сгенерировано $100$ штук. У данных с откликом $1$ (желтые) предикторы тоже независимы, но имеют распределение $\mathsf{N}_{2, 1}$. Всего сгенерировано $200$ значений, они изображены на рисунке \ref{data_dots}. 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{data.png}
\caption{Объекты интуитивно разделимы.}
\label{data_dots}
\end{figure}

Построим бинарное решающее дерево, ограничившись лишь двумя уровнями (деревом глубины $2$). Начальная энтропия рассматриваемого набора данных, конечно, максимальна и равна единице, так как у нас поровну единиц и нулей в отклике. Расчет в системе моделирования привел к тому, что для начального набора разделение будет оптимальным по признаку $X_{2} \leq 0.77$. Таким образом, выполняются действия, аналогичные рассмотренному примеру с породами кошек, однако заметно возрастает количество вычислений.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{tree.png}
\caption{Дерево решений.}
\label{derevo}
\end{figure}
На рисунке \ref{derevo} отражено сказанное нами ранее: исходный набор из $200$ элементов делится на два по признаку $X_{2} \leq 0.77$. При этом к зеленому блоку относятся те данные, которые удовлетворяют условию разделению, то есть данные, второй признак которых не превосходит $0.77$, а к красному -- все остальные. Так как энтропия в каждой группе отлична от нуля, то алгоритм продолжает свою работу. Видно, что в обеих группах следующее разделение идет по признаку $X_1$. Левая группа делится по условию $X_1 \leq 1.59$, а правая -- по условию $X_1 \leq 0.04$.

В данном примере глубина ограничена 2 уровнями, но, как мы видим, даже у такого <<ограниченного>> дерева есть группа с нулевой энтропией (самая левая), что, конечно, хорошо. Согласно сформулированному алгоритму, все узлы на втором уровне дерева становятся листами с откликами <<Зеленый>>, <<Желтый>>, <<Зеленый>>, <<Желтый>>, соответственно. Например, почему второй лист отвечает отклику <<Желтый>>? Как видно, из всех тренировочных данных в этот блок попадает $3$ зеленых и $7$ желтых объектов. Отклик же устанавливается по отклику наибольшего числа входящих объектов. Так как объектов с откликом <<Желтый>> большинство, то таков отклик и у листа.

Классификацию (а также сформулированные условия разделения) при помощи ДПР в нашем примере можно визуализировать непосредственно на плоскости. Для этого на первом этапе всю плоскость достаточно разделить прямой $X_2 = 0.77$. Мы видим на рисунке \ref{data_tree_line_1}, что плоскость разделилась на две части: нижняя, отвечающая левому блоку в дереве ($X_2 \leq 0.77$) и верхняя, отвечающая правому блоку ($X_2 > 0.77$). 

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{data_tree_line_1.png}
\caption{Классификация на основе дерева решений (первое разделение).}
\label{data_tree_line_1}
\end{figure}

Далее, условие следующего разделения для нижней части плоскости (левого блока в дереве) -- это условие $X_1 \leq 1.59$, а для верхней (или правого блока) -- условие $X_1 \leq 0.04$. Визуализацию разделения вы видите на рисунке \ref{data_tree_line_3}.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{data_tree_line_3.png}
\caption{Классификация на основе дерева решений (итоговое разделение).}
\label{data_tree_line_3}
\end{figure}

На этом этапе дерево считается обученным. Мы видим, что оно допускает ошибки и на тренировочных данных -- это ситуации, когда желтые точки оказываются в зеленой области, или, наоборот, когда зеленые точки попадают в желтую область; всего таких ошибок -- $17$. Посмотрим, как наше дерево будет работать на новой выборке из нормального распределения с такими же параметрами. Как видно, в целом, число ошибок сохранилось: на этапе обучения их было $17$, а на этапе тестирования -- $21$. На интуитивном уровне, дерево ошибается примерно в $11 \%$ случаев, что не так уж и плохо, а значит дерева с двумя уровнями вполне достаточно.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{data_tree_test.png}
\caption{Классификация новых объектов на основе обученной модели.}
\end{figure}
Но что произойдет, если увеличить количество уровней, например, до шести? Конечно, дерево станет заметно больше, но и число групп с нулевой энтропией возрастет. На рисунке изображены области классификации, отвечающие условиям деления на группы при построении дерева на все тех же исходных данных, как и при построении дерева с двумя уровнями.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{data_tree_6.png}
\caption{Классификация на основе дерева решений (6 уровней).}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{data_tree_6d_test.png}
\caption{Классификация новых объектов на основе обученной модели.}
\end{figure}

Похоже, исходя из наглядного представления дерева, что мы получили переобучение: и число областей, и их причудливая форма кажутся совершенно неестественными. И это действительно так. Протестировав наше дерево на тестовом наборе, мы видим, что имеются как пустые области, которые не используются вовсе (в них не попадает ни одно из тестовых данных), так и области, в которые теперь попадает много точек из другого класса -- ошибки классификации.  
